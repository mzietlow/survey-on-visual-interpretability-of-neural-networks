\begin{abstract}
    In this paper we give a theoretical overview of methods for understanding non-linear predictors (especially Neural Networks) visually. Our focus lies on heatmapping methods that try to assign relevances to each input-variable individually. From our literature-review, we derive a taxonomy of gradient\= and perturbation-based approaches. We provide an overview of commonly used metrics for evaluating the efficiency of such heatmapping-methods. Further, we describe a set of saliency-checks for assessing the reliability for a given metric.
\end{abstract}
\section{Taxonomy}
Visual interpretability of \gls{nn} as discussed in this survey finds itself as a subset of the \textit{explainable artificial intelligence}-research sector. Alternative approaches are rule-extraction\= or intrinsic methods~\cite{ras2018explanation}. The field of visual interpretability itself can be broken into multiple subsets, for example \fcite{vanDerMaaten2008} propose to build a mapping of latent-represantations of input-samples via dimensionality reduction onto a 2D-Plane. However, in this work we are mainly concered with generating heatmaps (\cref{subsect:heatmap}) for input-samples. We distinguish between Gradient-Based and Perturbation-Based methods, which we found to dominate the literature in this area of research.

\subsection{Gradient-Based} Gradient-Based approaches generate heatmaps by observing the change in model-output \(f(x)\) with respect to an input-variable \(x_i\). In this subsection, the set of gradient-based approaches will be further split into~\ref{itm:function},~\ref{itm:signal}, and~\ref{itm:attribution} approaches. 
\subsubsection{Functions, Signal and Attribution} In the past years, a large variety of gradient-based methods have been proposed (e.g.~\cite{Bach.2015,Zeiler.2014,Springenberg.2015,Kindermans.2018,Lundberg.2017,Gurumoorthy.2017,Simonyan.2014,DanielSmilkov.,Sundararajan.2017,Landecker.2013,Zhang.2018,Shrikumar.,Selvaraju.2017}). Broadly speaking, each method explains either~\ref{itm:function},~\ref{itm:signal}, or~\ref{itm:attribution}~\cite{Kindermans.2018,Kindermans.2019}. Usually, this is done by \textit{projecting} the model-output \(f(x)\) back into the input-space \(\mathbb{R}^{|x|}\) of input-sample \(x\), with \(x\in \mathbb{R}^{|x|}\).
\begin{description}
    \item[\namedlabel{itm:function}{Function}{}] approximates the amount by which an infinitesimal change in each dimension (in the following: \textit{variable}) \(i\) in the input-sample \(x\) influences the model-output \(f(x)\). The strength and direction are then simply the partial derivative with respect to the input-variable \(x_i\), i.e.\ \(\frac{\partial f(x)}{\partial x_i}\). However, as function-approaches are usually considered to be only poorly informatory~\cite{Kindermans.2018} and are only used as a baseline, they will not be detailed in this work. For details, please refer to, for example, \fcite{Simonyan.2014}.
    \item[\namedlabel{itm:signal}{Signal}{}] tries to reveal which input-variables caused the prediction~\cite{Kindermans.2018,Zeiler.2014} Signal-approaches are, to the best of our knowledge, constrained to \glspl{cnn}. Signal-approaches include for example DeConvNet~\cite{Zeiler.2014} and Guided Backpropagation~\cref{subsect:guided-backprop}, which have been shown to only approximate patterns that cause increased neuron-activity in higher layers~\cite{Kindermans.2019} (i.e.\ layers close to the output-layer). A more recent approach to this is PatternNet~\cref{subsect:patternnet-attribution}
    \item[\namedlabel{itm:attribution}{Attribution}{}] tries to approximate the contribution of each input-value \(x_i\in x\). This is usually called either \textit{attribution} or \textit{relevance}. Approaches to this are SHAP~\cite{Lundberg.2017}, ProtoDash~\cite{Gurumoorthy.2017}, \gls{lrp} \cref{subsect:lrp}, \gls{td} \cref{subsect:td}, \gls{dtd} \cref{subsect:dtd}, and PatternAttribution \cref{subsect:patternnet-attribution}.
\end{description}

\subsection{Perturbation-Based}
Not only Gradient-Based, but many Perturbation-Based methods were developed in the last years (\cite{AmitDhurandhar.2018,Ribeiro.2018,Luss.,Kim.2018,Zintgraf.2017}). They visualize how parts of the input data correspond with certain features and which influence they have on the given output. This is done by altering the input in certain ways and measuring the differences. Results are visualized in either a diagram (Prediction Difference Analysis: \cref{subsect:PDA}, Anchors: \cref{subsect:anchors}) or a heatmap (LIME: \cref{subsect:lime}). Some methods like LIME (\cref{subsect:lime}) are even able, to work with text as well as pictures and can present their results in both, diagrams and heatmaps. 
\par
The Contrastive Explanations Method (\cref{subsect:CEM}) uses an own approach and visualizes \glspl{pp} and \glspl{pn}. What these are is explained later. Activation Maximization (\cref{subsect:ActivationMax}) computes an input for a certain feature, that activates the feature the most, here the output is an image.
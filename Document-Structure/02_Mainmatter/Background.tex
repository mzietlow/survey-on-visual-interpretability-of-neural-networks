\section{Background}
\blindtext[3]

\subsection{What is interpretability?}
According to Leilani et al. “ The goal of interpretability is to describe the internals of a system in a way that is understandable to humans”. For this an explanation must be found, that is simple enough and uses meaningful vocabulary for a person. In addition the bias and knowledge must be considered.
Velez et al. use a similar definition of Interpretability that is adapted to machine learning: Interpretability is “the ability to explain or to present in understandable terms to a human”. Velez at al. also mentions, that not all ML systems need to be interpretable, because not all systems require human-machine-interaction. But especially in today's society, where more and more ML is used, humans want to understand the decisions for safety and ethical reasons.


\subsection{Why is interpretability a necessity?}
\blindtext[3]

\subsection{What is an explanation?}
There are multiple views on what an explanation actually is. For this paper a scientific definition is chosen. An explanation focuses on explaining the process inside a neural network and answers the question “Why does this particular input lead to that particular output?”. (Gilpin p.2)

\subsection{What is a heatmap?}
For visualizing the decisions of a neural network most researchers are using heatmaps. Heatmaps describe, which parts of the input data are most important for the output. A heatmap can exist in various forms, e.g. as a list of objects, which is sorted by relevance or for visual images a image, which shows the most important pixels by color. Acona et al. also mentions, that most often the task of one neuron is researched. Such maps are called attribution maps, in which red and blue markers indicate if the point of data (pixel) contributes positively or negativly to the feature.
\section{Background}
In this section the general motivation for this paper is described and definitions for interpretability, explanations and heatmaps are found.

\subsection{What is interpretability?}
According to \fcite{Gilpin.2018} “The goal of interpretability is to describe the internals of a system in a way that is understandable to humans”. For this an explanation must be found, that is simple enough and uses meaningful vocabulary for a person. In addition the bias and knowledge must be considered.
\fcite{DoshiVelez.2017} use a similar definition of Interpretability that is adapted to machine learning: Interpretability is “the ability to explain or to present in understandable terms to a human”. \fcite{DoshiVelez.2017} also mention, that not all ML systems need to be interpretable, because not all systems require human-machine-interaction. 
/par
But especially in today's society, where more and more ML is used, humans want to understand the decisions for safety and ethical reasons. They will not use something they do not understand or trust.
\fcite{DoshiVelez.2017} also mention, that for knowledge gain a scientific understanding is indispensable.

\subsection{What is an explanation?}
There are multiple views on what an explanation actually is. For this paper a scientific definition is chosen. An explanation focuses on explaining the process inside a neural network and answers the question “Why does this particular input lead to that particular output?”~\cite[2]{Gilpin.2018}

\subsection{What is a heatmap?} 
For visualizing the decisions of a neural network most researchers are using heatmaps. Heatmaps describe, which parts of the input data are most important for the output. A heatmap can exist in various forms, e.g.\ as a list of objects, which is sorted by relevance or for visual images a image, which shows the most important pixels by color. \Cite{Acona.2018} also mention, that most often the task of one neuron is researched. Such maps are called attribution maps, in which red and blue markers indicate if the point of data (pixel) contributes positively or negatively to the feature.

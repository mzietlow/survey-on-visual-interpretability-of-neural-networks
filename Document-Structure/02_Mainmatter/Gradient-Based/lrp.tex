\subsection{Layer-Wise Relevance Decomposition}
\begin{figure*}[h]
    \center
    \includegraphics[]{lrp-fig2.PNG}
    \caption[Single-Layer Neural Network; prediction and propagation via \gls{lrp}]{\textbf{Left:} A neural network at prediction time. \textbf{Right:} a neural network at relevance-propagation time, adopted from \protect\cite{Bach.2015}}\label{fig:lrp-nn}
\end{figure*}
\gls{lrp} attempts to provide a measure for the \nameref{subsubsect:pixel-wise-decomp}, discussed earlier in \cref{subsubsect:pixel-wise-decomp}. The relevances are computed \textit{backwards} from the output of the model (\(f(x)\)) to its input (\(x\)) after prediction time. Relevance is denoted as \(R_{i}^{l}\), the relevance of neuron \(i\) in layer \(l\), where \(l=1\) is the input layer. \gls{lrp} is formulated by \fcite{Bach.2015} as a set of constraints:
\begin{enumerate}
    \item \gls{lrp} must follow \ref{subsubsect:pixel-wise-decomp} completely.\label[const]{enum:gls-1}
    \item each layer of the neural network can be represented by relevances, such that 
    \begin{multline}
        f(x) = \dots \\ 
        = \sum_{j\in l+1} R_j^{(l+1)} = \sum_{i\in l} R_i^{(l)} \\
        = \dots = \sum_{h\in x} R_h^{(1)}.\label{eq:lw-propagation}
    \end{multline}
    Informally, \cref{eq:lw-propagation} implies that relevances can be propagated from the predictor-output \(f(x)\) to the input-variables in \(x\).\label[const]{enum:gls-2}
    \item the relevance of the \textit{output neuron} is defined as the model-output \(f(x)\).\label[const]{enum:gls-3}
    \item the relevance of \textit{every other neuron} is the sum of its incoming \glspl{message}, formally 
    \begin{equation}
        R_i^{(l)} = \sum_{\left\{ j | \text{Path } P_{i \rightarrow j} \text{ exists}\right\}} R_{i\leftarrow j}^{(l, l+1)}.\label[constraint]{eq:incoming-messages}
    \end{equation}\label[const]{enum:gls-4}
    \item the total relevance that a neuron sends out as \glspl{message} is equal to its own relevance, formally
    \begin{equation}
        R_j^{(l+1)} = \sum_{\left\{ i | \text{Path } P_{i \rightarrow j} \text{ exists}\right\}} R_{i\leftarrow j}^{(l, l+1)}.
    \end{equation}\label[const]{enum:gls-5}
    \item each \gls{message} \(R_{i\leftarrow j}^{(l,l+1)}\) is the product of the relevance of neuron \(j\), \(R_j^{(l+1)}\), weighted by the \textit{relative strength} of the path \(P_{i\rightarrow j}\), formally
    \begin{equation}
        R_{i\leftarrow j}^{(l,l+1)} = R_{j}^{(l+1)} \frac{a_i w_{ij}}{\sum_{h\in (l)} a_h w_{hj} + b_j},\label{eq:weighted-message}
    \end{equation}
    where \(a_i\) is the output of neuron \(i\) (precisely the output of its activation-function), \(w_ij\) are the weights that connect neurons \(i \text{ and } j\) and \(b_j\) is the bias for layer \(j\). 
    \par
    \Cref{eq:weighted-message} shows the simplest case of a \textit{weighting}-term. Different approaches for stabilizing the propagation, such as \(\alpha\beta\)-\gls{lrp} and \(\epsilon\)-\gls{lrp}, are further discussed in~\cite[20-22]{Bach.2015}.\label[const]{enum:gls-6}
\end{enumerate}
In summary, \gls{lrp} requires a conservation of relevance between layers (\cref{enum:gls-2}), the relevance of a neuron to be both the sum of its incoming weighted \glspl{message} (\cref{enum:gls-4,enum:gls-6}) and the sum of its outgoing weighted \glspl{message} (\cref{enum:gls-5,enum:gls-6}).

\subsection{Taylor-Decomposition}
As an approximation to \gls{lrp}, \gls{td} was proposed in~\cite{Bach.2015}. It directly defines relevances \(R_{h}^{(1)}\) at the (slightly modified) input-sample \(x'\) and therefore avoids the process of decomposing relevances for each layer. Instead, relevance \(R_{h}^{(1)}\) is defined by developing a first order Taylor series of the input \(x\) around a root point \(x_0\), such that \(f(x_0) = 0\). Informally, \gls{td} attributes relevance by weighting the difference of the image and the root point (\(x-x_0\)) with the partial derivative of the predictor with respect to the input \(x\). Formally
\begin{equation}
    R_{h}^{(1)} \approx (x-x_0)_{(h)} * \frac{\partial f}{\partial x_{(h)}}(x_0).
\end{equation}
Here, \(x'\) as mentioned above is \(x':=x-x_0\).
\par
Again, a set of constraints is formulated to find the root-point \(x_0\)
\begin{enumerate}
    \item \(x_0\) must be a root point of \(f\), such that \(f(x_0)=0\)
    \item \(x_0\) must be in the neighborhood of \(x\) under some distance metric, e.g.\ the Euclidean L2-norm.\label[const]{enum:taylor-closeness}
\end{enumerate}
A major issue with \gls{td} is satisfying \cref{enum:taylor-closeness}. \fcite{Bach.2015} do not provide a solution for sparsely populated data-domains, e.g.\ datasets of natural images. Yet, when the root point is ill-defined, \fcite{Kindermans.2019} were able to show that \gls{td} produces bad results.

\subsection{Deep-Taylor-Decomposition}\label{subsect:dtd}
\gls{dtd} as proposed by \fcite{Montavon.2017} tries to overcome the difficulties that the search for root-points according to \cref{enum:taylor-closeness} posed to simple \gls{td}. For this, they introduce axioms \ref{ax:positivity} and \ref{ax:consistency} which \gls{dtd} must satisfy. Also, they propose a method for finding root-points \(x_0(i):= x_i + t*v_i\), where \(v_i\) is called \textit{search direction}. \citeauthor{Montavon.2017} use this definition to define a set of weighting terms, similar to those in \gls{lrp} and \gls{td}. Formally, they introduce a generalization of the weighting parameter in \cref{eq:weighting-in-dtd}\cite[see][Supplementary Material]{Montavon.2017}
\begin{equation}
    R_{i\leftarrow j}^{(l,l+1)} = \sum_j \frac{v_i w_{ij}}{\sum_i v_i w_{ij}} R_j^{(l+1)}\label{eq:weighting-in-dtd}
\end{equation}
and then derive specific weighting parameters by defining \(v_i\).
\begin{description}
    \item[\namedlabel{itm:w2weighting}{\(\symbfit{w^2}\)-Weighting}{}] which is similar to \cref{eq:weighted-message} but does only rely on the weights \(w_{ij}\) that connect \(i \text{ and } j\). \(v_i\) is chosen to be \(v_i:=w_{ij}\).
    \begin{equation}
        R_{i\leftarrow j}^{(l,l+1)} = R_{j}^{(l+1)} \frac{w_{ij}^2}{\sum_{h\in (l)} w_{hj}^2}.\label{eq:w2-weighting-dtd}
    \end{equation}
    \(w^2\)-Weighting is used, when the input-domain is unrestricted, i.e.\ \(x\in \mathbb R^{|x|}\).
    \item[\namedlabel{itm:z+weighting}{\(\symbfit{z^+}\)-Weighting}{}] is equal to the \(\alpha\beta\)-Rule of \gls{lrp}, with \(\beta=0, \alpha=1\). Here, \(v_i\) is the set of all \(x_i\) for which the correspoding \(w_{ij}\) is positive, formally \(w_{ij}^{+} = \left\{w_{ij} | w_{ij}\geq 0\right\}\) and \(v_i:=\left\{x_i|w_{ij}\in w_{ij}^{+}\right\}\). The weighting therefore is
    \begin{equation}
        R_{i\leftarrow j}^{(l,l+1)} = R_{j}^{(l+1)} \frac{x_i w_{ij}^{+}}{\sum_{h\in (l)} x_(h) w_{hj}^{+}}.\label{eq:dtd-z+}
    \end{equation}
    In~\cite{Montavon.2017}, \cref{eq:dtd-z+} is modeled with \(x_i w_{ij}^{+}=:z_{ij}^{+}\), from which the name \(z^{+}\)-Weighting is derived.
    \(z^{+}\)-Weighting is used, when the input-domain is restricted to positive values, for example in a setting with ReLU-activations, i.e.\ \(x\in \mathbb R_{+}^{|x|}\). A generalization of the \(z^{+}\)-Weighting, as discussed in~\cite{Montavon.2017}, is equal to \(\alpha\beta\)-\gls{lrp}.
    \item[\(\symbfit{z^{\mathscr B}}\)-Weighting] is used, when the input-domain is restricted to an upper- and lower-bound, for example in image-classification tasks. For details, please refer to~\cite[215\psq]{Montavon.2017}
\end{description}
\par
In contrast to \gls{td}, \gls{dtd} defines not immediately the relevance at the input-sample \(R_{h}^{(1)}\), but only the relevance of \glspl{message}. It therefore incorporates ideas of both \gls{lrp} (relevance-propagation, even partial equality with the generalization of \ref{itm:z+weighting}) and \gls{td} (approximation of function value with respect to a root-point \(x_0\)). The relevance at the input-sample is recieved by a relevance model that then propagates the \glspl{message} from \(f(x)\) to \(R_{h}^{(1)}\).
\paragraph{Relevance-Models}
\begin{figure*}[h]
    \center
    \includegraphics[width=\textwidth]{dtd-flow}
    \caption[Deeper Neural Network; prediction and propagation via \gls{dtd}]{\textbf{Left:} A neural network at prediction time. \textbf{Right:} exemplary \gls{dtd} Relevance Model, adopted from \protect\cite{Montavon.2017}. Note, that in this work, usually \(R_h\) is used instead of \(R_p\).}\label{fig:dtd-nn}
\end{figure*}
\fcite{Montavon.2017} propose two relevance models (\textit{Min-Max Relevance Model} and \textit{Training-free Relevance Model}), for which details are provided in~\cite{Montavon.2017}. They are presented as an inversion of the original \gls{nn}, incorporating their structure. However, no explicit constraints on this property are formulated. The flow in a relevance model is visually depicted in \cref{fig:dtd-nn}. First a prediction \(f(x)=:x_f\) is made (left side) which is then fed through the Relevance Model which extends \(R_f:=x_f\) back to the input sample. While the \textit{Training-free Relevance Model} must, as its name suggests, not be trained, the \textit{Min-Max Relevance Model} has to be trained in a supervised fashion. \fcite{Montavon.2017} show that both, the training-free and min-max relevance models, lead to very similar results.
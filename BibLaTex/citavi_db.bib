% This file was created with Citavi 6.3.0.0

@proceedings{.2013,
 year = {2013},
 title = {2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)}
}


@misc{Chen.20180614,
 author = {Chen, Jianbo and {Le Song} and Wainwright, Martin J. and Jordan, Michael I.},
 date = {2018},
 title = {Learning to Explain: An Information-Theoretic Perspective on Model Interpretation},
 file = {Chen, Song et al 2018 - Learning to Explain - An Information-Theoretic Perspective on Model Interpretation:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Chen, Song et al 2018 - Learning to Explain - An Information-Theoretic Perspective on Model Interpretation.pdf:pdf}
}


@inproceedings{Pang.2019,
 author = {Pang, Yanwei and Xie, Jin and Muhammed, Khan Haris and Rao, Anwer Muhammed and Fahad, Khan Shahbaz and Shao, Ling},
 title = {Mask-Guided Attention Network for Occluded Pedestrian Detection},
 url = {https://arxiv.org/abs/1910.06160},
 urldate = {2019-12-11},
 pages = {4967 - 4965},
 bookpagination = {page},
 booktitle = {ICCV 2019},
 year = {2019},
 file = {2019 - Pang_Mask-Guided_Attention_Network_for_Occluded_Pedestrian_Detection_ICCV_2019_paper:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\2019 - Pang_Mask-Guided_Attention_Network_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.pdf:pdf}
}


@article{Montavon.2018,
 abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
 author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
 year = {2018},
 title = {Methods for interpreting and understanding deep neural networks},
 url = {http://www.sciencedirect.com/science/article/pii/S1051200417302385},
 keywords = {Activation maximization;Deep neural networks;Layer-wise relevance propagation;Sensitivity analysis;Taylor decomposition},
 pages = {1--15},
 pagination = {page},
 volume = {73},
 issn = {1051-2004},
 journal = {Digital Signal Processing},
 doi = {10.1016/j.dsp.2017.10.011}
}


@misc{AdityaChattopadhyay.2019,
 author = {{Aditya Chattopadhyay} and {Piyushi Manupriya} and {Anirban Sarkar} and {Vineeth N Balasubramanian}},
 year = {2019},
 title = {Neural Network Attributions: A Causal Perspective}
}


@proceedings{.2019,
 year = {2019},
 title = {NeurIPS 2019}
}


@book{I.Guyon.2017,
 year = {2017},
 title = {NIPS~2017: Advances in Neural Information Processing Systems 30},
 publisher = {{Curran Associates, Inc}},
 editor = {{I. Guyon} and {U. V. Luxburg} and {S. Bengio} and {H. Wallach} and {R. Fergus} and {S. Vishwanathan} and {R. Garnett}}
}


@proceedings{.2018,
 year = {2018},
 title = {NIPS 2018: Proceedings of the 32nd International Conference on Neural Information Processing Systems},
 address = {USA},
 publisher = {{Curran Associates Inc}},
 series = {NIPS’18}
}


@misc{Shrikumar.,
 author = {Shrikumar, Avanti and Greenside, Peyton and Shcherbina, Anna and Kundaje, Anshul},
 date = {2016},
 title = {Not Just a Black Box: Learning Important Features Through Propagating Activation Differences},
 file = {Shrikumar, Greenside et al 2017 - Not Just a Black Box - Learning Important Features Through Propagating Activation Differences:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Shrikumar, Greenside et al 2017 - Not Just a Black Box - Learning Important Features Through Propagating Activation Differences.pdf:pdf}
}


@article{Bach.2015,
 abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
 author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
 year = {2015},
 title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
 urldate = {2019-12-08},
 volume = {10},
 number = {7},
 issn = {1932-6203},
 journal = {PLOS ONE},
 doi = {10.1371/journal.pone.0130140},
 file = {Bach, Binder et al 2015 - On Pixel-Wise Explanations for Non-Linear:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Bach, Binder et al 2015 - On Pixel-Wise Explanations for Non-Linear.pdf:pdf}
}


@proceedings{.2016,
 year = {2016},
 title = {RepL4NLP: Proceedings of the 1st Workshop on Representation Learning for NLP},
 urldate = {2019-12-08}
}


@inproceedings{Adebayo.2018,
 author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
 title = {Sanity Checks for Saliency Maps},
 pages = {9525--9536},
 bookpagination = {page},
 publisher = {{Curran Associates Inc}},
 series = {NIPS’18},
 booktitle = {NIPS 2018},
 year = {2018},
 address = {USA},
 file = {Adebayo, Gilmer et al 2018 - Sanity Checks for Saliency Maps:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Adebayo, Gilmer et al 2018 - Sanity Checks for Saliency Maps.pdf:pdf}
}


@misc{Tomsett.2019,
 author = {Tomsett, Richard and Harborne, Dan and Chakraborty, Supriyo and Gurram, Prudhvi and Preece, Alun},
 date = {2019},
 title = {Sanity Checks for Saliency Metrics},
 file = {Tomsett, Harborne et al 2019 - Sanity Checks for Saliency Metrics:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Tomsett, Harborne et al 2019 - Sanity Checks for Saliency Metrics.pdf:pdf}
}


@misc{DanielSmilkov.,
 author = {{Daniel Smilkov} and {Nikhil Thorat} and {Been Kim} and {Fernanda Viégas} and {Martin Wattenberg}},
 date = {2017},
 title = {SmoothGrad: removing noise by adding noise},
 file = {Smilkov, Thorat et al 2017 - SmoothGrad - removing noise by adding noise:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Smilkov, Thorat et al 2017 - SmoothGrad - removing noise by adding noise.pdf:pdf}
}


@misc{EthanR.Elenberg.2017,
 author = {{Ethan R. Elenberg} and {Alexandros G. Dimakis} and {Moran Feldman} and {Amin Karbasi}},
 year = {2017},
 title = {Streaming Weak Submodularity: Interpreting Neural Networks on the Fly}
}


@inproceedings{Springenberg.2015,
 author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
 title = {Striving for Simplicity: The All Convolutional Net},
 booktitle = {ICLR 2015},
 year = {2015},
 file = {Springenberg, Dosovitskiy et al 2015 - Striving for Simplicity - The All Convolutional Net:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Springenberg, Dosovitskiy et al 2015 - Striving for Simplicity - The All Convolutional Net.pdf:pdf}
}


@incollection{Kindermans.2019,
 abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step which can be compensated for easily‐-adding a constant shift to the input data‐-to show that a transformation with no effect on how the model makes the decision can cause numerous methods to attribute incorrectly. In order to guarantee reliability, we believe that the explanation should not change when we can guarantee that two networks process the images in identical manners. We show, through several examples, that saliency methods that do not satisfy this requirement result in misleading attribution. The approach can be seen as a type of unit test; we construct a narrow ground truth to measure one stated desirable property. As such, we hope the community will embrace the development of additional tests.},
 author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
 title = {The (Un)reliability of Saliency Methods},
 pages = {267--280},
 bookpagination = {page},
 publisher = {{Springer International Publishing} and Springer},
 isbn = {978-3-030-28954-6},
 series = {Lecture Notes in Artificial Intelligence},
 editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert},
 booktitle = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
 year = {2019},
 address = {Cham}
}


@article{Zhang.2018,
 author = {Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
 year = {2018},
 title = {Top-Down Neural Attention by Excitation Backprop},
 url = {https://doi.org/10.1007/s11263-017-1059-x},
 keywords = {Convolutional Neural Network;Selective tuning;Top-down attention},
 pages = {1084--1102},
 pagination = {page},
 volume = {126},
 number = {10},
 issn = {0920-5691},
 journal = {Int. J. Comput. Vision},
 file = {Jianming, Lin et al 2016 - Top-Down Neural Attention by Excitation Backprop:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Jianming, Lin et al 2016 - Top-Down Neural Attention by Excitation Backprop.pdf:pdf}
}


@inproceedings{Acona.2018,
 author = {Acona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
 title = {Towards Better Understanding of Gradient-Based  Attribution Methods for Deep Neural Networks},
 url = {https://www.researchgate.net/publication/321124808_A_unified_view_of_gradient-based_attribution_methods_for_Deep_Neural_Networks},
 urldate = {2019-12-11},
 booktitle = {ICLR 2018},
 year = {2018},
 file = {72796aeca8605b2e370d8a756a7a311fd171:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\72796aeca8605b2e370d8a756a7a311fd171.pdf:pdf}
}


@misc{Ancona.,
 author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
 date = {2017},
 title = {Towards better understanding of gradient-based attribution methods for Deep Neural Networks},
 file = {Ancona, Ceolini et al 2018 - Towards better understanding of gradient-based attribution methods for Deep Neural Networks:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Ancona, Ceolini et al 2018 - Towards better understanding of gradient-based attribution methods for Deep Neural Networks.pdf:pdf}
}


@inproceedings{AlvarezMelis.2018,
 author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
 title = {Towards Robust Interpretability with Self-explaining Neural Networks},
 url = {http://dl.acm.org/citation.cfm?id=3327757.3327875},
 pages = {7786--7795},
 bookpagination = {page},
 publisher = {{Curran Associates Inc}},
 series = {NIPS’18},
 booktitle = {NIPS 2018},
 year = {2018},
 address = {USA}
}


@article{McGaughey.2016,
 abstract = {Three (3) different methods (logistic regression, covariate shift and k-NN) were applied to five (5) internal datasets and one (1) external, publically available dataset where covariate shift existed. In all cases, k-NN's performance was inferior to either logistic regression or covariate shift. Surprisingly, there was no obvious advantage for using covariate shift to reweight the training data in the examined datasets.},
 author = {McGaughey, Georgia and Walters, W. Patrick and Goldman, Brian},
 year = {2016},
 title = {Understanding covariate shift in model performance},
 volume = {5},
 issn = {2046-1402},
 journal = {F1000Research},
 doi = {10.12688/f1000research.8317.3},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27803797},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5070592}
}


@inproceedings{AvantiShrikumar.2017,
 author = {{Avanti Shrikumar} and {Peyton Greenside} and {Anshul Kundaje}},
 title = {Learning Important Features Through Propagating Activation Differences},
 pages = {3145--3153},
 bookpagination = {page},
 volume = {70},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {{Doina Precup} and {Yee Whye Teh}},
 booktitle = {ICML 2017},
 year = {2017},
 address = {International Convention Centre, Sydney, Australia},
 file = {Shrikumar, Greenside et al 2017 - Learning Important Features Through Propagating Activation Differences:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Shrikumar, Greenside et al 2017 - Learning Important Features Through Propagating Activation Differences.pdf:pdf}
}


@inproceedings{Kindermans.2018,
 author = {Kindermans, Pieter-Jan and Schütt, Kristof T. and Alber, Maximilian and Müller, Klaus-Robert and Erhan, Dumitru and Kim, Been and Dähne, Sven},
 title = {Learning how to explain neural networks: PatternNet and PatternAttribution},
 urldate = {2018-02-15},
 booktitle = {ICLR 2018},
 year = {2018},
 file = {Kindermans, Schütt et al 2018 - Learning how to explain neural:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Kindermans, Schütt et al 2018 - Learning how to explain neural.pdf:pdf}
}


@inproceedings{Landecker.2013,
 author = {Landecker, W. and Thomure, M. D. and Bettencourt, L. M. A. and Mitchell, M. and Kenyon, G. T. and Brumby, S. P.},
 title = {Interpreting individual classifications of hierarchical networks},
 keywords = {Accuracy;classification accuracy;contribution propagation;data classification;data sets;Equations;hierarchical networks;Kernel;learning (artificial intelligence);machine-learning tasks;Mathematical model;network behavior;network classifications;pattern classification;per-instance explanations;Shape;Support vector machines;Vectors;visual object-recognition tasks},
 pages = {32--38},
 bookpagination = {page},
 booktitle = {2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)},
 year = {2013},
 file = {Landecker, Thomure et al 2013 - Interpreting individual classifications of hierarchical networks:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Landecker, Thomure et al 2013 - Interpreting individual classifications of hierarchical networks.pdf:pdf}
}


@proceedings{Chaudhuri.2019,
 year = {2019},
 title = {ICML 2019: Proceedings of the 36th International Conference on Machine Learning},
 address = {Long Beach, California, USA},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan}
}


@inproceedings{Hooker.2019,
 author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
 title = {A Benchmark for Interpretability Methods in Deep Neural Networks},
 booktitle = {NeurIPS 2019},
 year = {2019}
}


@incollection{Lundberg.2017,
 author = {Lundberg, Scott M. and Lee, Su-In},
 title = {A Unified Approach to Interpreting Model Predictions},
 pages = {4765--4774},
 bookpagination = {page},
 publisher = {{Curran Associates, Inc}},
 editor = {{I. Guyon} and {U. V. Luxburg} and {S. Bengio} and {H. Wallach} and {R. Fergus} and {S. Vishwanathan} and {R. Garnett}},
 booktitle = {NIPS~2017},
 year = {2017},
 file = {Lundberg and Lee 2017 - A Unified Approach to Interpreting Model Predictions:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Lundberg and Lee 2017 - A Unified Approach to Interpreting Model Predictions.pdf:pdf}
}


@inproceedings{Bach.2016,
 author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 title = {Analyzing Classifiers: Fisher Vectors and Deep Neural Networks},
 urldate = {2019-12-08},
 pages = {2912--2920},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 booktitle = {CVPR2016},
 year = {2016},
 address = {[S.l.]},
 file = {Bach et al 2016 - Analyzing Classifiers - Fisher Vectors and Deep Neural Networks:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Bach et al 2016 - Analyzing Classifiers - Fisher Vectors and Deep Neural Networks.pdf:pdf}
}


@inproceedings{Sundararajan.2017,
 abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
 author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
 title = {Axiomatic Attribution for Deep Networks},
 pages = {3319--3328},
 bookpagination = {page},
 volume = {70},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {{Doina Precup} and {Yee Whye Teh}},
 booktitle = {ICML 2017},
 year = {2017},
 address = {International Convention Centre, Sydney, Australia}
}


@inproceedings{Wang.2019,
 abstract = {The gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features and has been widely studied to assist in interpreting DNNs. In a linear model (i.e., g(x) = wx + b), the gradient corresponds to the weights w. Such a model can reasonably locally-linearly approximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The bias b, however, is usually overlooked in attribution methods. In this paper, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of predictions, it can also play a significant role in understanding DNN behavior. We propose a backpropagation-type algorithm “bias back-propagation (BBp)” that starts at the output layer and iteratively attributes the bias of each layer to its input nodes as well as combining the resulting bias term of the previous layer. Together with the backpropagation of the gradient generating w, we can fully recover the locally linear model g(x) = wx + b. In experiments, we show that BBp can generate complementary and highly interpretable explanations.},
 author = {Wang, Shengjie and Zhou, Tianyi and Bilmes, Jeff},
 title = {Bias Also Matters: Bias Attribution for Deep Neural Network Explanation},
 url = {http://proceedings.mlr.press/v97/wang19p.html},
 pages = {6659--6667},
 bookpagination = {page},
 volume = {97},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
 booktitle = {ICML 2019},
 year = {2019},
 address = {Long Beach, California, USA}
}


@proceedings{.2016b,
 year = {2016},
 title = {CVPR2016: IEEE Conference on Computer Vision and Pattern Recognition},
 address = {[S.l.]},
 urldate = {2019-12-08},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 file = {http://www.worldcat.org/oclc/1096688412}
}


@misc{Simonyan.20140419,
 author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
 date = {2013},
 title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
 file = {Simonyan, Vedaldi et al 2013 - Deep Inside Convolutional Networks - Visualising Image Classification Models and Saliency Maps:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Simonyan, Vedaldi et al 2013 - Deep Inside Convolutional Networks - Visualising Image Classification Models and Saliency Maps.pdf:pdf}
}


@proceedings{Fleet.2014,
 year = {2014},
 title = {ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014: Proceedings, Part I.},
 address = {Cham},
 volume = {8689},
 publisher = {Springer},
 isbn = {978-3-319-10590-1},
 series = {Lecture notes in computer science},
 editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
 file = {http://www.worldcat.org/oclc/897211509}
}


@article{Gurumoorthy.2017,
 author = {Gurumoorthy, Karthik S. and Dhurandhar, Amit and Cecchi, Guillermo and Aggarwal, Charu},
 year = {2017},
 title = {Efficient Data Representation by Selecting Prototypes with Importance Weights},
 file = {Gurumoorthy, Dhurandhar et al 2019 - Efficient Data Representation by Selecting Prototypes with Importance Weights:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Gurumoorthy, Dhurandhar et al 2019 - Efficient Data Representation by Selecting Prototypes with Importance Weights.pdf:pdf}
}


@article{WojciechSamek.2015,
 author = {{Wojciech Samek} and {Alexander Binder} and {Grégoire Montavon} and {Sebastian Lapuschkin} and {Klaus-Robert Müller}},
 year = {2015},
 title = {Evaluating the Visualization of What a Deep Neural Network Has Learned},
 pages = {2660--2673},
 pagination = {page},
 volume = {28},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 file = {Samek, Binder et al 2015 - Evaluating the visualization of what a Deep Neural Network as learned:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Samek, Binder et al 2015 - Evaluating the visualization of what a Deep Neural Network as learned.pdf:pdf}
}


@inproceedings{Zeiler.2014,
 abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
 author = {Zeiler, Matthew D. and Fergus, Rob},
 title = {Visualizing and Understanding Convolutional Networks},
 publisher = {Springer},
 isbn = {978-3-319-10590-1},
 series = {Lecture notes in computer science},
 editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
 booktitle = {ECCV 2014: Proceedings, Part I.},
 year = {2014},
 address = {Cham},
 file = {Zeiler and Fergus 2014 - Visualizing and Understanding Convolutional Networks:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Zeiler and Fergus 2014 - Visualizing and Understanding Convolutional Networks.pdf:pdf}
}


@book{Samek.2019,
 year = {2019},
 title = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
 address = {Cham},
 urldate = {2019-12-08},
 edition = {1st ed. 2019},
 volume = {11700},
 publisher = {{Springer International Publishing} and Springer},
 isbn = {978-3-030-28954-6},
 series = {Lecture Notes in Artificial Intelligence},
 editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert},
 file = {Samek, Montavon et al 2019 - Explainable AI - Interpreting, Explaining and Visualizing Deep Learning:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Samek, Montavon et al 2019 - Explainable AI - Interpreting, Explaining and Visualizing Deep Learning.pdf:pdf}
}


@inproceedings{Arras.2016,
 author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 title = {Explaining Predictions of Non-Linear Classifiers in NLP},
 urldate = {2019-12-08},
 pages = {1--7},
 bookpagination = {page},
 booktitle = {RepL4NLP},
 year = {2016},
 file = {Arras, Horn et al 2016 - Explaining Predictions of Non-Linear Classifiers:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Arras, Horn et al 2016 - Explaining Predictions of Non-Linear Classifiers.pdf:pdf}
}


@misc{AmitDhurandhar.2018,
 author = {{Amit Dhurandhar} and {Pin-Yu Chen} and {Ronny Luss} and {Chun-Chen Tu} and {Paishun Ting} and {Karthikeyan Shanmugam} and {Payel Das}},
 year = {2018},
 title = {Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives}
}


@article{Selvaraju.2019,
 abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach‐-Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265‐290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E.},
 author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
 year = {2019},
 title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 file = {Selvaraju, Cogswell et al 2019 - Grad-CAM Visual Explanations from Deep Networks via Gradient-Based Localization:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Selvaraju, Cogswell et al 2019 - Grad-CAM Visual Explanations from Deep Networks via Gradient-Based Localization.pdf:pdf}
}


@article{Selvaraju.2019b,
 abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach‐-Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265‐290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E.},
 author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
 year = {2019},
 title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision}
}


@inproceedings{Selvaraju.2017,
 author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
 title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
 keywords = {Cats;CNN;CNN model-families;coarse localization map;Computer architecture;convolution;Convolutional Neural Network;data visualisation;deep networks;Dogs;fine-grained visualizations;Grad- CAM;Grad-CAM explanations;gradient methods;gradient-based localization;Gradient-weighted Class Activation Mapping;Guided Grad-CAM;high-resolution class-discriminative visualization;image captioning;image classification;image classification models;image representation;inference mechanisms;Knowledge discovery;learning (artificial intelligence);neural nets;nonattention based models;object detection;object recognition;reinforcement learning;visual explanations;visual question answering models;Visualization},
 pages = {618--626},
 bookpagination = {page},
 booktitle = {ICCV 2017},
 year = {2017}
}


@proceedings{.2017,
 year = {2017},
 title = {ICCV 2017: IEEE International Conference on Computer Vision}
}


@proceedings{.2019b,
 year = {2019},
 title = {ICCV 2019: IEEE International Conference on Computer Vision}
}


@proceedings{.2015,
 year = {2015},
 title = {ICLR 2015: 3rd International Conference on Learning Representations (Workshop Track)}
}


@proceedings{.2018b,
 year = {2018},
 title = {ICLR 2018: 6th International Conference on Learning Representations}
}


@proceedings{DoinaPrecup.2017,
 year = {2017},
 title = {ICML 2017: Proceedings of the 34th International Conference on Machine Learning},
 url = {http://proceedings.mlr.press/v70/},
 address = {International Convention Centre, Sydney, Australia},
 volume = {70},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {{Doina Precup} and {Yee Whye Teh}}
}


@book{JairEscalante.2018,
 year = {2018},
 title = {Explainable and Interpretable Models in Computer Vision and Machine Learning},
 address = {[S.l.]},
 urldate = {2019-12-08},
 publisher = {{SPRINGER INTERNATIONAL PU}},
 isbn = {978-3-319-98131-4},
 editor = {{Jair Escalante}, Hugo and {Escalera, Sergkl, Guyon, Isabelle} and Baró, Xavier and Güçlütürk, Yağmur and {Güçlü, Umut, van Gerven, Marcel}},
 file = {Escalante, Escalera et al 2018 - Explainable and Interpretable Models in Computer Vision and Machine Learning:Z\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Escalante, Escalera et al 2018 - Explainable and Interpretable Models in Computer Vision and Machine Learning.pdf:pdf}
}


@misc{CS231n.,
 author = {CS231n, Stanford},
 title = {Visualizing what ConvNets learn},
 url = {https://cs231n.github.io/understanding-cnn/}
}



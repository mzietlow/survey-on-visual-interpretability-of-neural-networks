% This file was created with Citavi 6.3.0.0

@book{.,
 title = {CoRR 2019}
}


@inproceedings{Lipton.2016,
 author = {Lipton, Zachary},
 title = {The Mythos of Model Interpretability},
 url = {https://arxiv.org/pdf/1606.03490.pdf},
 urldate = {2019-12-19},
 booktitle = {ICML 2016},
 year = {2016}
}


@inproceedings{Li.2016,
 author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
 title = {Understanding Neural Networks through Representation Erasure},
 url = {https://arxiv.org/pdf/1612.08220.pdf},
 urldate = {2019-12-23},
 booktitle = {CoRR},
 year = {2016}
}


@inproceedings{Le.2012,
 author = {Le, Quoc and Marc, Ranzato Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg and Dean, Jeff and Ng, Andrew},
 title = {Building High-level Features Using Large Scale Unsupervised Learning},
 url = {https://icml.cc/2012/papers/73.pdf},
 urldate = {2019-12-23},
 booktitle = {29th International Conference on Machine Learning},
 year = {2012}
}


@inproceedings{Lapuschkin.2019,
 author = {Lapuschkin, Sebastian and Wäldchen, Stephan and Binder, Alexander and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
 title = {Unmasking Clever Hans Predictors and Assessing What Machines Really Learn},
 url = {https://arxiv.org/pdf/1902.10178.pdf},
 urldate = {2019-12-20},
 booktitle = {Nature Communications},
 year = {2019}
}


@inproceedings{Landecker.2013,
 author = {Landecker, W. and Thomure, M. D. and Bettencourt, L. M. A. and Mitchell, M. and Kenyon, G. T. and Brumby, S. P.},
 title = {Interpreting individual classifications of hierarchical networks},
 keywords = {Accuracy;classification accuracy;contribution propagation;data classification;data sets;Equations;hierarchical networks;Kernel;learning (artificial intelligence);machine-learning tasks;Mathematical model;network behavior;network classifications;pattern classification;per-instance explanations;Shape;Support vector machines;Vectors;visual object-recognition tasks},
 pages = {32--38},
 bookpagination = {page},
 booktitle = {2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)},
 year = {2013},
 file = {Landecker, Thomure et al 2013 - Interpreting individual classifications of hierarchical networks:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Landecker, Thomure et al 2013 - Interpreting individual classifications of hierarchical networks.pdf:pdf}
}


@inproceedings{Koh.2017,
 author = {Koh, Pang Wei and Liang, Percy},
 title = {Understanding Black-box Predictions via Influence Functions},
 url = {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf},
 urldate = {2019-12-23},
 booktitle = {34 th International Conference on Machine Learning},
 year = {2017}
}


@inproceedings{Kindermans.2018,
 author = {Kindermans, Pieter-Jan and Schütt, Kristof T. and Alber, Maximilian and Müller, Klaus-Robert and Erhan, Dumitru and Kim, Been and Dähne, Sven},
 title = {Learning how to explain neural networks: PatternNet and PatternAttribution},
 urldate = {2018-02-15},
 booktitle = {ICLR 2018},
 year = {2018},
 file = {Kindermans, Schütt et al 2018 - Learning how to explain neural:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Kindermans, Schütt et al 2018 - Learning how to explain neural.pdf:pdf}
}


@incollection{Kindermans.2019,
 abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step which can be compensated for easily‐-adding a constant shift to the input data‐-to show that a transformation with no effect on how the model makes the decision can cause numerous methods to attribute incorrectly. In order to guarantee reliability, we believe that the explanation should not change when we can guarantee that two networks process the images in identical manners. We show, through several examples, that saliency methods that do not satisfy this requirement result in misleading attribution. The approach can be seen as a type of unit test; we construct a narrow ground truth to measure one stated desirable property. As such, we hope the community will embrace the development of additional tests.},
 author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
 title = {The (Un)reliability of Saliency Methods},
 pages = {267--280},
 bookpagination = {page},
 publisher = {{Springer International Publishing} and Springer},
 isbn = {978-3-030-28954-6},
 series = {Lecture Notes in Artificial Intelligence},
 editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert},
 booktitle = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
 year = {2019},
 address = {Cham}
}


@incollection{Lundberg.2017,
 author = {Lundberg, Scott M. and Lee, Su-In},
 title = {A Unified Approach to Interpreting Model Predictions},
 pages = {4765--4774},
 bookpagination = {page},
 publisher = {{Curran Associates, Inc}},
 editor = {{I. Guyon} and {U. V. Luxburg} and {S. Bengio} and {H. Wallach} and {R. Fergus} and {S. Vishwanathan} and {R. Garnett}},
 booktitle = {NIPS~2017},
 year = {2017},
 file = {Lundberg and Lee 2017 - A Unified Approach to Interpreting Model Predictions:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Lundberg and Lee 2017 - A Unified Approach to Interpreting Model Predictions.pdf:pdf}
}


@inproceedings{Kim.2018,
 author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
 title = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)},
 url = {https://arxiv.org/pdf/1711.11279.pdf},
 urldate = {2019-12-22},
 booktitle = {ICML 2018},
 year = {2018}
}


@book{I.Guyon.2017,
 year = {2017},
 title = {NIPS~2017: Advances in Neural Information Processing Systems 30},
 publisher = {{Curran Associates, Inc}},
 editor = {{I. Guyon} and {U. V. Luxburg} and {S. Bengio} and {H. Wallach} and {R. Fergus} and {S. Vishwanathan} and {R. Garnett}}
}


@inproceedings{Hooker.2019,
 author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
 title = {A Benchmark for Interpretability Methods in Deep Neural Networks},
 booktitle = {NeurIPS 2019},
 year = {2019}
}


@article{Gurumoorthy.2017,
 author = {Gurumoorthy, Karthik S. and Dhurandhar, Amit and Cecchi, Guillermo and Aggarwal, Charu},
 year = {2017},
 title = {Efficient Data Representation by Selecting Prototypes with Importance Weights},
 file = {Gurumoorthy, Dhurandhar et al 2019 - Efficient Data Representation by Selecting Prototypes with Importance Weights:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Gurumoorthy, Dhurandhar et al 2019 - Efficient Data Representation by Selecting Prototypes with Importance Weights.pdf:pdf}
}


@misc{Gu.,
 author = {Gu, Jindong and Tresp, Volker},
 title = {Contextual Prediction Difference Analysis},
 url = {https://arxiv.org/pdf/1910.09086.pdf},
 urldate = {2019-12-20}
}


@inproceedings{Gilpin.2018,
 author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
 title = {Explaining Explanations: An Overview of Interpretability of Machine Learning},
 pages = {80--89},
 bookpagination = {page},
 publisher = {{Conference Publishing Services, IEEE Computer Society}},
 isbn = {978-1-5386-5090-5},
 editor = {Bonchi, Francesco},
 booktitle = {2018 IEEE 5th International Conference on Data Science and Advanced Analytics},
 year = {2018},
 address = {Los Alamitos, California},
 doi = {10.1109/DSAA.2018.00018},
 file = {https://ieeexplore.ieee.org/document/8631448/}
}


@proceedings{Fleet.2014,
 year = {2014},
 title = {ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014: Proceedings, Part I.},
 address = {Cham},
 volume = {8689},
 publisher = {Springer},
 isbn = {978-3-319-10590-1},
 series = {Lecture notes in computer science},
 editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
 file = {http://www.worldcat.org/oclc/897211509}
}


@book{Fleet.2014b,
 year = {2014},
 title = {Computer vision - ECCV 2014: 13th European conference, Zurich, Switzerland, September 6 - 12, 2014; proceedings},
 address = {Cham},
 volume = {8689},
 publisher = {Springer},
 isbn = {978-3-319-10589-5},
 series = {Lecture notes in computer science},
 editor = {Fleet, David},
 doi = {10.1007/978-3-319-10590-1},
 file = {http://www.gbv.de/dms/tib-ub-hannover/798855681.pdf}
}


@misc{EthanR.Elenberg.2017,
 author = {{Ethan R. Elenberg} and {Alexandros G. Dimakis} and {Moran Feldman} and {Amin Karbasi}},
 year = {2017},
 title = {Streaming Weak Submodularity: Interpreting Neural Networks on the Fly}
}


@book{JairEscalante.2018,
 year = {2018},
 title = {Explainable and Interpretable Models in Computer Vision and Machine Learning},
 address = {[S.l.]},
 urldate = {2019-12-08},
 publisher = {{SPRINGER INTERNATIONAL PU}},
 isbn = {978-3-319-98131-4},
 editor = {{Jair Escalante}, Hugo and {Escalera, Sergkl, Guyon, Isabelle} and Baró, Xavier and Güçlütürk, Yağmur and {Güçlü, Umut, van Gerven, Marcel}},
 file = {Escalante, Escalera et al 2018 - Explainable and Interpretable Models in Computer Vision and Machine Learning:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Escalante, Escalera et al 2018 - Explainable and Interpretable Models in Computer Vision and Machine Learning.pdf:pdf}
}


@incollection{Luss.,
 author = {Luss, Ronny and Chen, Pin-Yu and Dhurandhar, Amit and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Tu, Chun-Chen},
 title = {Generating Contrastive Explanations with Monotonic Attribute Functions},
 url = {https://arxiv.org/pdf/1905.12698.pdf},
 urldate = {2019-12-23},
 booktitle = {CoRR 2019}
}


@article{Mahendran.2016,
 author = {Mahendran, Aravindh and Vedaldi, Andrea},
 year = {2016},
 title = {Visualizing Deep Convolutional Neural Networks Using Natural Pre-images},
 pages = {233--255},
 pagination = {page},
 volume = {120},
 number = {3},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 doi = {10.1007/s11263-016-0911-8}
}


@article{McGaughey.2016,
 abstract = {Three (3) different methods (logistic regression, covariate shift and k-NN) were applied to five (5) internal datasets and one (1) external, publically available dataset where covariate shift existed. In all cases, k-NN's performance was inferior to either logistic regression or covariate shift. Surprisingly, there was no obvious advantage for using covariate shift to reweight the training data in the examined datasets.},
 author = {McGaughey, Georgia and Walters, W. Patrick and Goldman, Brian},
 year = {2016},
 title = {Understanding covariate shift in model performance},
 volume = {5},
 issn = {2046-1402},
 journal = {F1000Research},
 doi = {10.12688/f1000research.8317.3},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5070592},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27803797}
}


@inproceedings{Zeiler.2014,
 abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
 author = {Zeiler, Matthew D. and Fergus, Rob},
 title = {Visualizing and Understanding Convolutional Networks},
 publisher = {Springer},
 isbn = {978-3-319-10590-1},
 series = {Lecture notes in computer science},
 editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
 booktitle = {ECCV 2014: Proceedings, Part I.},
 year = {2014},
 address = {Cham},
 file = {Zeiler and Fergus 2014 - Visualizing and Understanding Convolutional Networks:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Zeiler and Fergus 2014 - Visualizing and Understanding Convolutional Networks.pdf:pdf}
}


@article{WojciechSamek.2015,
 author = {{Wojciech Samek} and {Alexander Binder} and {Grégoire Montavon} and {Sebastian Lapuschkin} and {Klaus-Robert Müller}},
 year = {2015},
 title = {Evaluating the Visualization of What a Deep Neural Network Has Learned},
 pages = {2660--2673},
 pagination = {page},
 volume = {28},
 journal = {IEEE Transactions on Neural Networks and Learning Systems},
 file = {Samek, Binder et al 2015 - Evaluating the visualization of what a Deep Neural Network as learned:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Samek, Binder et al 2015 - Evaluating the visualization of what a Deep Neural Network as learned.pdf:pdf}
}


@inproceedings{Wang.2019,
 abstract = {The gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features and has been widely studied to assist in interpreting DNNs. In a linear model (i.e., g(x) = wx + b), the gradient corresponds to the weights w. Such a model can reasonably locally-linearly approximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The bias b, however, is usually overlooked in attribution methods. In this paper, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of predictions, it can also play a significant role in understanding DNN behavior. We propose a backpropagation-type algorithm “bias back-propagation (BBp)” that starts at the output layer and iteratively attributes the bias of each layer to its input nodes as well as combining the resulting bias term of the previous layer. Together with the backpropagation of the gradient generating w, we can fully recover the locally linear model g(x) = wx + b. In experiments, we show that BBp can generate complementary and highly interpretable explanations.},
 author = {Wang, Shengjie and Zhou, Tianyi and Bilmes, Jeff},
 title = {Bias Also Matters: Bias Attribution for Deep Neural Network Explanation},
 url = {http://proceedings.mlr.press/v97/wang19p.html},
 pages = {6659--6667},
 bookpagination = {page},
 volume = {97},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
 booktitle = {ICML 2019},
 year = {2019},
 address = {Long Beach, California, USA}
}


@misc{Tomsett.2019,
 author = {Tomsett, Richard and Harborne, Dan and Chakraborty, Supriyo and Gurram, Prudhvi and Preece, Alun},
 date = {2019},
 title = {Sanity Checks for Saliency Metrics},
 file = {Tomsett, Harborne et al 2019 - Sanity Checks for Saliency Metrics:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Tomsett, Harborne et al 2019 - Sanity Checks for Saliency Metrics.pdf:pdf}
}


@inproceedings{Sundararajan.2017,
 abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
 author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
 title = {Axiomatic Attribution for Deep Networks},
 pages = {3319--3328},
 bookpagination = {page},
 volume = {70},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {{Doina Precup} and {Yee Whye Teh}},
 booktitle = {ICML 2017},
 year = {2017},
 address = {International Convention Centre, Sydney, Australia}
}


@inproceedings{Springenberg.2015,
 author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
 title = {Striving for Simplicity: The All Convolutional Net},
 booktitle = {ICLR 2015},
 year = {2015},
 file = {Springenberg, Dosovitskiy et al 2015 - Striving for Simplicity - The All Convolutional Net:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Springenberg, Dosovitskiy et al 2015 - Striving for Simplicity - The All Convolutional Net.pdf:pdf}
}


@inproceedings{Simonyan.2014,
 author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
 title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
 url = {https://arxiv.org/pdf/1312.6034.pdf},
 urldate = {2019-12-20},
 booktitle = {ICLR 2014},
 year = {2014}
}


@inproceedings{shu.2019,
 author = {shu, Hai and Zhu, Hongtu},
 title = {Sensitivity Analysis of Deep Neural Networks},
 url = {https://arxiv.org/pdf/1901.07152.pdf},
 urldate = {2019-12-23},
 booktitle = {AAAI Conference on Artificial Intelligence 2019},
 year = {2019}
}


@misc{Shrikumar.,
 author = {Shrikumar, Avanti and Greenside, Peyton and Shcherbina, Anna and Kundaje, Anshul},
 date = {2016},
 title = {Not Just a Black Box: Learning Important Features Through Propagating Activation Differences},
 file = {Shrikumar, Greenside et al 2017 - Not Just a Black Box - Learning Important Features Through Propagating Activation Differences:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Shrikumar, Greenside et al 2017 - Not Just a Black Box - Learning Important Features Through Propagating Activation Differences.pdf:pdf}
}


@article{Selvaraju.2019,
 abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach‐-Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265‐290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E.},
 author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
 year = {2019},
 title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision}
}


@article{Selvaraju.2019b,
 abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach‐-Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265‐290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E.},
 author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
 year = {2019},
 title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
 issn = {1573-1405},
 journal = {International Journal of Computer Vision},
 file = {Selvaraju, Cogswell et al 2019 - Grad-CAM Visual Explanations from Deep Networks via Gradient-Based Localization:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Selvaraju, Cogswell et al 2019 - Grad-CAM Visual Explanations from Deep Networks via Gradient-Based Localization.pdf:pdf}
}


@inproceedings{Selvaraju.2017,
 author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
 title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
 keywords = {Cats;CNN;CNN model-families;coarse localization map;Computer architecture;convolution;Convolutional Neural Network;data visualisation;deep networks;Dogs;fine-grained visualizations;Grad- CAM;Grad-CAM explanations;gradient methods;gradient-based localization;Gradient-weighted Class Activation Mapping;Guided Grad-CAM;high-resolution class-discriminative visualization;image captioning;image classification;image classification models;image representation;inference mechanisms;Knowledge discovery;learning (artificial intelligence);neural nets;nonattention based models;object detection;object recognition;reinforcement learning;visual explanations;visual question answering models;Visualization},
 pages = {618--626},
 bookpagination = {page},
 booktitle = {ICCV 2017},
 year = {2017}
}


@book{Samek.2019,
 year = {2019},
 title = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
 address = {Cham},
 urldate = {2019-12-08},
 edition = {1st ed. 2019},
 volume = {11700},
 publisher = {{Springer International Publishing} and Springer},
 isbn = {978-3-030-28954-6},
 series = {Lecture Notes in Artificial Intelligence},
 editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert},
 file = {Samek, Montavon et al 2019 - Explainable AI - Interpreting, Explaining and Visualizing Deep Learning:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Samek, Montavon et al 2019 - Explainable AI - Interpreting, Explaining and Visualizing Deep Learning.pdf:pdf}
}


@inproceedings{RobnikSikonja.2008,
 author = {Robnik-Sikonja, Marko and Kononenko, Igor},
 title = {Explaining Classifications for Individual Instances},
 url = {http://lkm.fri.uni-lj.si/rmarko/papers/RobnikSikonjaKononenko08-TKDE.pdf},
 urldate = {2019-12-23},
 booktitle = {IEEE Transactions on Knowledge and Data Engineering 2008},
 year = {2008}
}


@inproceedings{Ribeiro.2018,
 author = {Ribeiro, Marco and Singh, Sameer and Guestrin, Carlos},
 title = {Anchors: High-Precision Model-Agnostic Explanations},
 url = {https://homes.cs.washington.edu/~marcotcr/aaai18.pdf},
 urldate = {2019-12-23},
 booktitle = {32nd AAAI Conference on Artificial Intelligence},
 year = {2018}
}


@inproceedings{Ribeiro.2016,
 author = {Ribeiro, Marco and Singh, Sameer and Guestrin, Carlos},
 title = {“Why Should I Trust You?” Explaining the Predictions of Any Classifier},
 url = {https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf},
 urldate = {2019-12-23},
 booktitle = {KDD '16 Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 year = {2016}
}


@inproceedings{Pang.2019,
 author = {Pang, Yanwei and Xie, Jin and Muhammed, Khan Haris and Rao, Anwer Muhammed and Fahad, Khan Shahbaz and Shao, Ling},
 title = {Mask-Guided Attention Network for Occluded Pedestrian Detection},
 url = {https://arxiv.org/abs/1910.06160},
 urldate = {2019-12-11},
 pages = {4967 - 4965},
 bookpagination = {page},
 booktitle = {ICCV 2019},
 year = {2019},
 file = {2019 - Pang_Mask-Guided_Attention_Network_for_Occluded_Pedestrian_Detection_ICCV_2019_paper:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\2019 - Pang_Mask-Guided_Attention_Network_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.pdf:pdf}
}


@inproceedings{Nguyen.2016,
 author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
 title = {Synthesizing the preferred inputs for neurons in neural networks via deep generator networks},
 url = {https://arxiv.org/pdf/1605.09304.pdf},
 urldate = {2019-12-22},
 pages = {3395--3403},
 bookpagination = {page},
 booktitle = {30th International Conference on Neural Information Processing Systems},
 year = {2016}
}


@article{Montavon.2018,
 abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
 author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
 year = {2018},
 title = {Methods for interpreting and understanding deep neural networks},
 url = {http://www.sciencedirect.com/science/article/pii/S1051200417302385},
 keywords = {Activation maximization;Deep neural networks;Layer-wise relevance propagation;Sensitivity analysis;Taylor decomposition},
 pages = {1--15},
 pagination = {page},
 volume = {73},
 issn = {1051-2004},
 journal = {Digital Signal Processing},
 doi = {10.1016/j.dsp.2017.10.011}
}


@misc{Erhan.2009,
 author = {Erhan, Dumitru and Yoshua, Yoshua and Courville, Aaron and Vincent, Pascal},
 year = {2009},
 title = {Visualizing Higher-Layer Features of a Deep Network},
 url = {https://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf},
 urldate = {2019-12-22}
}


@misc{DoshiVelez.2017,
 author = {Doshi-Velez, Finale and Kim, Been},
 year = {2017},
 title = {Towards A Rigorous Science of Interpretable Machine Learning},
 url = {https://arxiv.org/pdf/1702.08608.pdf},
 urldate = {2019-12-20}
}


@proceedings{DoinaPrecup.2017,
 year = {2017},
 title = {ICML 2017: Proceedings of the 34th International Conference on Machine Learning},
 url = {http://proceedings.mlr.press/v70/},
 address = {International Convention Centre, Sydney, Australia},
 volume = {70},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {{Doina Precup} and {Yee Whye Teh}}
}


@inproceedings{Dhurandhar.2018,
 author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan},
 title = {Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives},
 url = {https://researcher.watson.ibm.com/researcher/files/us-adhuran/explanations-based-missing-final.pdf},
 urldate = {2019-12-23},
 booktitle = {32nd Conference on Neural Information Processing Systems (NIPS 2018)},
 year = {2018}
}


@proceedings{.2018,
 year = {2018},
 title = {ICML 2018}
}


@proceedings{.2018b,
 year = {2018},
 title = {ICLR 2018: 6th International Conference on Learning Representations}
}


@proceedings{.2018c,
 year = {2018},
 title = {32nd Conference on Neural Information Processing Systems (NIPS 2018)}
}


@proceedings{.2018d,
 year = {2018},
 title = {32nd AAAI Conference on Artificial Intelligence}
}


@proceedings{.2017,
 year = {2017},
 title = {ICLR 2017}
}


@proceedings{.2017b,
 year = {2017},
 title = {ICCV 2017: IEEE International Conference on Computer Vision}
}


@proceedings{.2017c,
 year = {2017},
 title = {CVPR 2017}
}


@proceedings{.2017d,
 year = {2017},
 title = {34 th International Conference on Machine Learning}
}


@proceedings{.2016,
 year = {2016},
 title = {RepL4NLP: Proceedings of the 1st Workshop on Representation Learning for NLP},
 urldate = {2019-12-08}
}


@proceedings{.2016b,
 year = {2016},
 title = {KDD '16 Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}
}


@proceedings{.2016c,
 year = {2016},
 title = {ICML 2016}
}


@proceedings{.2016d,
 year = {2016},
 title = {CVPR2016: IEEE Conference on Computer Vision and Pattern Recognition},
 address = {[S.l.]},
 urldate = {2019-12-08},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 file = {http://www.worldcat.org/oclc/1096688412}
}


@proceedings{.2016e,
 year = {2016},
 title = {CoRR}
}


@proceedings{.2016f,
 year = {2016},
 title = {30th International Conference on Neural Information Processing Systems}
}


@proceedings{.2015,
 year = {2015},
 title = {ICLR 2015: 3rd International Conference on Learning Representations (Workshop Track)}
}


@proceedings{.2014,
 year = {2014},
 title = {ICLR 2014}
}


@proceedings{.2013,
 year = {2013},
 title = {2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)}
}


@proceedings{.2012,
 year = {2012},
 title = {29th International Conference on Machine Learning}
}


@proceedings{.2008,
 year = {2008},
 title = {IEEE Transactions on Knowledge and Data Engineering 2008}
}


@proceedings{.2018e,
 year = {2018},
 title = {NIPS 2018: Proceedings of the 32nd International Conference on Neural Information Processing Systems},
 address = {USA},
 publisher = {{Curran Associates Inc}},
 series = {NIPS’18}
}


@article{Zhang.2018,
 author = {Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
 year = {2018},
 title = {Top-Down Neural Attention by Excitation Backprop},
 url = {https://doi.org/10.1007/s11263-017-1059-x},
 keywords = {Convolutional Neural Network;Selective tuning;Top-down attention},
 pages = {1084--1102},
 pagination = {page},
 volume = {126},
 number = {10},
 issn = {0920-5691},
 journal = {Int. J. Comput. Vision},
 file = {Jianming, Lin et al 2016 - Top-Down Neural Attention by Excitation Backprop:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Jianming, Lin et al 2016 - Top-Down Neural Attention by Excitation Backprop.pdf:pdf}
}


@proceedings{.2019,
 year = {2019},
 title = {AAAI Conference on Artificial Intelligence 2019}
}


@proceedings{.2019b,
 year = {2019},
 title = {ICCV 2019: IEEE International Conference on Computer Vision}
}


@misc{DanielSmilkov.,
 author = {{Daniel Smilkov} and {Nikhil Thorat} and {Been Kim} and {Fernanda Viégas} and {Martin Wattenberg}},
 date = {2017},
 title = {SmoothGrad: removing noise by adding noise},
 file = {Smilkov, Thorat et al 2017 - SmoothGrad - removing noise by adding noise:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Smilkov, Thorat et al 2017 - SmoothGrad - removing noise by adding noise.pdf:pdf}
}


@misc{CS231n.,
 author = {CS231n, Stanford},
 title = {Visualizing what ConvNets learn},
 url = {https://cs231n.github.io/understanding-cnn/}
}


@proceedings{CoRR2019.2019,
 year = {2019},
 editor = {{CoRR 2019}}
}


@misc{Chen.20180614,
 author = {Chen, Jianbo and {Le Song} and Wainwright, Martin J. and Jordan, Michael I.},
 date = {2018},
 title = {Learning to Explain: An Information-Theoretic Perspective on Model Interpretation},
 file = {Chen, Song et al 2018 - Learning to Explain - An Information-Theoretic Perspective on Model Interpretation:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Chen, Song et al 2018 - Learning to Explain - An Information-Theoretic Perspective on Model Interpretation.pdf:pdf}
}


@proceedings{Chaudhuri.2019,
 year = {2019},
 title = {ICML 2019: Proceedings of the 36th International Conference on Machine Learning},
 address = {Long Beach, California, USA},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan}
}


@proceedings{Bonchi.2018,
 year = {2018},
 title = {2018 IEEE 5th International Conference on Data Science and Advanced Analytics: DSAA 2018 : proceedings : 1-4 October 2018, Turin, Italy},
 keywords = {Data mining;Data structures (Computer science);Database management},
 address = {Los Alamitos, California},
 publisher = {{Conference Publishing Services, IEEE Computer Society}},
 isbn = {978-1-5386-5090-5},
 editor = {Bonchi, Francesco},
 file = {http://www.worldcat.org/oclc/1101434890}
}


@inproceedings{Bau.2017,
 author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
 title = {Network Dissection: Quantifying Interpretability of Deep Visual Representations},
 url = {https://arxiv.org/pdf/1704.05796.pdf},
 urldate = {2019-12-23},
 booktitle = {CVPR 2017},
 year = {2017}
}


@inproceedings{Bach.2016,
 author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 title = {Analyzing Classifiers: Fisher Vectors and Deep Neural Networks},
 urldate = {2019-12-08},
 pages = {2912--2920},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 booktitle = {CVPR2016},
 year = {2016},
 address = {[S.l.]},
 file = {Bach et al 2016 - Analyzing Classifiers - Fisher Vectors and Deep Neural Networks:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Bach et al 2016 - Analyzing Classifiers - Fisher Vectors and Deep Neural Networks.pdf:pdf}
}


@article{Bach.2015,
 abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
 author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
 year = {2015},
 title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
 urldate = {2019-12-08},
 volume = {10},
 number = {7},
 issn = {1932-6203},
 journal = {PLOS ONE},
 doi = {10.1371/journal.pone.0130140},
 file = {Bach, Binder et al 2015 - On Pixel-Wise Explanations for Non-Linear:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Bach, Binder et al 2015 - On Pixel-Wise Explanations for Non-Linear.pdf:pdf}
}


@inproceedings{AvantiShrikumar.2017,
 author = {{Avanti Shrikumar} and {Peyton Greenside} and {Anshul Kundaje}},
 title = {Learning Important Features Through Propagating Activation Differences},
 pages = {3145--3153},
 bookpagination = {page},
 volume = {70},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {{Doina Precup} and {Yee Whye Teh}},
 booktitle = {ICML 2017},
 year = {2017},
 address = {International Convention Centre, Sydney, Australia},
 file = {Shrikumar, Greenside et al 2017 - Learning Important Features Through Propagating Activation Differences:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Shrikumar, Greenside et al 2017 - Learning Important Features Through Propagating Activation Differences.pdf:pdf}
}


@inproceedings{Arras.2016,
 author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
 title = {Explaining Predictions of Non-Linear Classifiers in NLP},
 urldate = {2019-12-08},
 pages = {1--7},
 bookpagination = {page},
 booktitle = {RepL4NLP},
 year = {2016},
 file = {Arras, Horn et al 2016 - Explaining Predictions of Non-Linear Classifiers:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Arras, Horn et al 2016 - Explaining Predictions of Non-Linear Classifiers.pdf:pdf}
}


@misc{Ancona.,
 author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
 date = {2017},
 title = {Towards better understanding of gradient-based attribution methods for Deep Neural Networks},
 file = {Ancona, Ceolini et al 2018 - Towards better understanding of gradient-based attribution methods for Deep Neural Networks:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Ancona, Ceolini et al 2018 - Towards better understanding of gradient-based attribution methods for Deep Neural Networks.pdf:pdf}
}


@misc{AmitDhurandhar.2018,
 author = {{Amit Dhurandhar} and {Pin-Yu Chen} and {Ronny Luss} and {Chun-Chen Tu} and {Paishun Ting} and {Karthikeyan Shanmugam} and {Payel Das}},
 year = {2018},
 title = {Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives}
}


@inproceedings{AlvarezMelis.2018,
 author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
 title = {Towards Robust Interpretability with Self-explaining Neural Networks},
 url = {http://dl.acm.org/citation.cfm?id=3327757.3327875},
 pages = {7786--7795},
 bookpagination = {page},
 publisher = {{Curran Associates Inc}},
 series = {NIPS’18},
 booktitle = {NIPS 2018},
 year = {2018},
 address = {USA}
}


@misc{AdityaChattopadhyay.2019,
 author = {{Aditya Chattopadhyay} and {Piyushi Manupriya} and {Anirban Sarkar} and {Vineeth N Balasubramanian}},
 year = {2019},
 title = {Neural Network Attributions: A Causal Perspective}
}


@inproceedings{Adebayo.2018,
 author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
 title = {Sanity Checks for Saliency Maps},
 pages = {9525--9536},
 bookpagination = {page},
 publisher = {{Curran Associates Inc}},
 series = {NIPS’18},
 booktitle = {NIPS 2018},
 year = {2018},
 address = {USA},
 file = {Adebayo, Gilmer et al 2018 - Sanity Checks for Saliency Maps:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\Adebayo, Gilmer et al 2018 - Sanity Checks for Saliency Maps.pdf:pdf}
}


@inproceedings{Acona.2018,
 author = {Acona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
 title = {Towards Better Understanding of Gradient-Based  Attribution Methods for Deep Neural Networks},
 url = {https://www.researchgate.net/publication/321124808_A_unified_view_of_gradient-based_attribution_methods_for_Deep_Neural_Networks},
 urldate = {2019-12-11},
 booktitle = {ICLR 2018},
 year = {2018},
 file = {72796aeca8605b2e370d8a756a7a311fd171:Y\:\\Citavi\\Survey - Visual Approaches Towards Interpretability Of Neural Networks\\Citavi Attachments\\72796aeca8605b2e370d8a756a7a311fd171.pdf:pdf}
}


@proceedings{.2019c,
 year = {2019},
 title = {NeurIPS 2019}
}


@proceedings{.2019d,
 year = {2019},
 title = {Nature Communications}
}


@proceedings{.2019e,
 year = {2019},
 title = {CoRR 2019}
}


@inproceedings{Zintgraf.2017,
 author = {Zintgraf, Luisa and Cohen, Taco and Adel, Tameem and Welling, Max},
 title = {VISUALIZING DEEP NEURAL NETWORK DECISIONS: PREDICTION DIFFERENCE ANALYSIS},
 url = {https://arxiv.org/pdf/1702.04595.pdf},
 urldate = {2019-12-23},
 booktitle = {ICLR 2017},
 year = {2017}
}



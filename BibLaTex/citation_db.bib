% This file was created with Citavi 6.3.0.0

@proceedings{.07.07.201310.07.2013,
 year = {2013-07-07/2013-07-10},
 title = {2013 IEEE Symposium on Computers and Communications (ISCC)},
 publisher = {IEEE},
 isbn = {978-1-4799-3755-4},
 abstract = {},
 venue = {Split, Croatia},
 eventdate = {2013-07-07/2013-07-10},
 eventtitle = {2013 IEEE Symposium on Computers and Communications (ISCC)}
}


@proceedings{.09.11.201715.11.2017,
 year = {2017-11-09/2017-11-15},
 title = {2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
 publisher = {IEEE},
 isbn = {978-1-5386-3586-5},
 abstract = {},
 venue = {Kyoto},
 eventdate = {2017-11-09/2017-11-15},
 eventtitle = {2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)}
}


@proceedings{.11.04.201614.04.2016,
 year = {2016-04-11/2016-04-14},
 title = {2016 12th IAPR Workshop on Document Analysis Systems (DAS)},
 publisher = {IEEE},
 isbn = {978-1-5090-1792-8},
 abstract = {},
 venue = {Santorini, Greece},
 eventdate = {2016-04-11/2016-04-14},
 eventtitle = {2016 12th IAPR Workshop on Document Analysis Systems (DAS)}
}


@proceedings{.16.12.201519.12.2015,
 year = {2015-12-16/2015-12-19},
 title = {2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)},
 publisher = {IEEE},
 isbn = {978-1-4673-8564-0},
 abstract = {},
 venue = {Patna, India},
 eventdate = {2015-12-16/2015-12-19},
 eventtitle = {2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)}
}


@proceedings{.18.09.201121.09.2011,
 year = {2011-09-18/2011-09-21},
 title = {2011 International Conference on Document Analysis and Recognition},
 publisher = {IEEE},
 isbn = {978-1-4577-1350-7},
 abstract = {},
 venue = {Beijing, China},
 eventdate = {2011-09-18/2011-09-21},
 eventtitle = {2011 International Conference on Document Analysis and Recognition (ICDAR)}
}


@proceedings{.1994,
 year = {1994},
 title = {NIPS},
 abstract = {}
}


@proceedings{.1995,
 year = {1995},
 abstract = {}
}


@proceedings{.20.10.200922.10.2009,
 year = {2009-10-20/2009-10-22},
 title = {2009 Eighth International Symposium on Natural Language Processing},
 publisher = {IEEE},
 isbn = {978-1-4244-4138-9},
 abstract = {},
 venue = {Bangkok, Thailand},
 eventdate = {2009-10-20/2009-10-22},
 eventtitle = {2009 Eighth International Symposium on Natural Language Processing (SNLP)}
}


@proceedings{.23.08.201526.08.2015,
 year = {2015-08-23/2015-08-26},
 title = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
 publisher = {IEEE},
 isbn = {978-1-4799-1805-8},
 abstract = {},
 venue = {Tunis, Tunisia},
 eventdate = {2015-08-23/2015-08-26},
 eventtitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)}
}


@proceedings{.23.09.200726.09.2007,
 year = {2007-09-23/2007-09-26},
 title = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2},
 publisher = {IEEE},
 isbn = {0-7695-2822-8},
 abstract = {},
 venue = {Curitiba, Parana, Brazil},
 eventdate = {2007-09-23/2007-09-26},
 eventtitle = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2}
}


@proceedings{.25.08.201328.08.2013,
 year = {2013-08-25/2013-08-28},
 title = {2013 12th International Conference on Document Analysis and Recognition},
 publisher = {IEEE},
 isbn = {978-0-7695-4999-6},
 abstract = {},
 venue = {Washington, DC, USA},
 eventdate = {2013-08-25/2013-08-28},
 eventtitle = {2013 12th International Conference on Document Analysis and Recognition (ICDAR)}
}


@proceedings{.814Dec.2001,
 year = {2001-12-08/2001-12-14},
 title = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
 publisher = {{IEEE Comput. Soc}},
 isbn = {0-7695-1272-0},
 abstract = {},
 venue = {Kauai, HI, USA},
 eventdate = {2001-12-08/2001-12-14},
 eventtitle = {2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}
}


@thesis{Ahmed.2016,
 author = {Ahmed, Sheraz},
 year = {2016},
 title = {A Generic Framework for Information Segmentation in Document Images: A part-based Approach},
 abstract = {A part-based feature refers to local properties of a part/local-area/patch of an image [68]. These properties can be based on change in intensity, color, texture, gradient, etc. Local features are usually distinct from their neighborhood. For example, corners of an object, usually exhibits different properties than the object itself.



To learn the properties of different types of information it is important to first have some sample images for each of the target information classes. Once these properties are learnt, feature banks containing features for each type information are generated. Later, the prediction uses these feature banks to segment different types of information available in previously unseen documents.},
 type = {Dissertation},
 institution = {{Technische Universität Kaiserslautern}},
 location = {Kaiserslautern},
 file = {https://kluedo.ub.uni-kl.de/frontdoor/index/index/docId/4508},
 file = {A Generic Framework for Information Segmentation in Document Images - A part-based Approach:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\A Generic Framework for Information Segmentation in Document Images - A part-based Approach.pdf:pdf},
 language = {eng}
}


@inproceedings{Ahmed.25.08.201328.08.2013,
 author = {Ahmed, Sheraz and Shafait, Faisal and Liwicki, Marcus and Dengel, Andreas},
 title = {A Generic Method for Stamp Segmentation Using Part-Based Features},
 pages = {708--712},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-0-7695-4999-6},
 booktitle = {2013 12th International Conference on Document Analysis and Recognition},
 year = {2013-08-25/2013-08-28},
 abstract = {Traditionally, stamps are considered as a seal of authenticity for documents. For automatic processing and verification, segmentation of stamps from documents is pivotal. Existing methods for stamp extraction mostly employ color and/or shape based techniques, thereby limiting their applicability to only colored and specific shape stamps. In this paper, a novel, generic method based on part-based features is presented for segmentation of stamps from document images. The proposed method can segment black, colored, unseen, arbitrary shaped, textual, as well as graphical stamps. The proposed method is evaluated on a publicly available dataset for stamp detection and verification and achieved recall and precision of 73{\%} and 83{\%} respectively, for black stamps which were not addressed in the past.},
 file = {A Generic Method for Stamp Segmentation Using Part-Based Features:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\A Generic Method for Stamp Segmentation Using Part-Based Features.pdf:pdf},
 eventtitle = {2013 12th International Conference on Document Analysis and Recognition (ICDAR)},
 venue = {Washington, DC, USA},
 eventdate = {2013-08-25/2013-08-28}
}


@misc{Alber.13.08.2018,
 author = {Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and Hägele, Miriam and Schütt, Kristof T. and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert and Dähne, Sven and Kindermans, Pieter-Jan},
 year = {13.08.2018},
 title = {iNNvestigate neural networks!},
 url = {http://arxiv.org/pdf/1808.04260v1},
 abstract = {In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.},
 file = {https://arxiv.org/pdf/1808.04260v1.pdf},
 file = {http://arxiv.org/abs/1808.04260v1}
}


@article{Bach.2015,
 author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
 year = {2015},
 title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
 pages = {e0130140},
 pagination = {page},
 volume = {10},
 journaltitle = {PloS one},
 language = {eng},
 number = {7},
 abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
 file = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation.PDF:PDF}
}


@misc{Bhalgat.16.09.2016,
 author = {Bhalgat, Yash and Kulkarni, Mandar and Karande, Shirish and Lodha, Sachin},
 year = {16.09.2016},
 title = {Stamp processing with examplar features},
 url = {http://arxiv.org/pdf/1609.05001v1},
 abstract = {Document digitization is becoming increasingly crucial. In this work, we propose a shape based approach for automatic stamp verification/detection in document images using an unsupervised feature learning. Given a small set of training images, our algorithm learns an appropriate shape representation using an unsupervised clustering. Experimental results demonstrate the effectiveness of our framework in challenging scenarios.},
 file = {Stamp processing with examplar features:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Stamp processing with examplar features.pdf:pdf}
}


@inproceedings{BillG.Horne.1994,
 author = {{Bill G. Horne} and {C. Lee Giles}},
 title = {An experimental comparison of recurrent neural networks},
 booktitle = {NIPS},
 year = {1994},
 abstract = {},
 file = {An experimental comparison of recurrent neural networks:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\An experimental comparison of recurrent neural networks.pdf:pdf}
}


@misc{Chen.15.11.2017,
 author = {Chen, Yining and Gilroy, Sorcha and Maletti, Andreas and May, Jonathan and Knight, Kevin},
 year = {15.11.2017},
 title = {Recurrent Neural Networks as Weighted Language Recognizers},
 abstract = {We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete and APX-hard. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.},
 file = {https://arxiv.org/pdf/1711.05408v2.pdf}
}


@article{Comrie.1997,
 author = {Comrie, Andrew C.},
 year = {1997},
 title = {Comparing Neural Networks and Regression Models for Ozone Forecasting},
 pages = {653--663},
 pagination = {page},
 volume = {47},
 issn = {1096-2247},
 journaltitle = {Journal of the Air {\&} Waste Management Association},
 doi = {10.1080/10473289.1997.10463925},
 number = {6},
 abstract = {},
 file = {Comparing Neural Networks and Regression Models for Ozone Forecasting:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Comparing Neural Networks and Regression Models for Ozone Forecasting.pdf:pdf}
}


@misc{Cui.07.02.2019,
 author = {Cui, Xinrui and Wang, Dan and Wang, Z. Jane},
 year = {07.02.2019},
 title = {CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks},
 abstract = {With the widespread applications of deep convolutional neural networks (DCNNs), it becomes increasingly important for DCNNs not only to make accurate predictions but also to explain how they make their decisions. In this work, we propose a CHannel-wise disentangled InterPretation (CHIP) model to give the visual interpretation to the predictions of DCNNs. The proposed model distills the class-discriminative importance of channels in networks by utilizing the sparse regularization. Here, we first introduce the network perturbation technique to learn the model. The proposed model is capable to not only distill the global perspective knowledge from networks but also present the class-discriminative visual interpretation for specific predictions of networks. It is noteworthy that the proposed model is able to interpret different layers of networks without re-training. By combining the distilled interpretation knowledge in different layers, we further propose the Refined CHIP visual interpretation that is both high-resolution and class-discriminative. Experimental results on the standard dataset demonstrate that the proposed model provides promising visual interpretation for the predictions of networks in image classification task compared with existing visual interpretation methods. Besides, the proposed method outperforms related approaches in the application of ILSVRC 2015 weakly-supervised localization task.},
 pagetotal = {15},
 file = {CHIP - Channel-wise Disentangled Interpretation of Deep Convolutional  Neural Networks:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\CHIP - Channel-wise Disentangled Interpretation of Deep Convolutional  Neural Networks.pdf:pdf}
}


@inproceedings{Dey.11.04.201614.04.2016,
 author = {Dey, Soumyadeep and Mukhopadhyay, Jayanta and Sural, Shamik},
 title = {Removal of Gray Rubber Stamps},
 pages = {210--214},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5090-1792-8},
 booktitle = {2016 12th IAPR Workshop on Document Analysis Systems (DAS)},
 year = {2016-04-11/2016-04-14},
 abstract = {},
 doi = {10.1109/DAS.2016.26},
 file = {http://ieeexplore.ieee.org/document/7490119/},
 file = {Removal of Gray Rubber Stamps:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Removal of Gray Rubber Stamps.pdf:pdf},
 eventtitle = {2016 12th IAPR Workshop on Document Analysis Systems (DAS)},
 venue = {Santorini, Greece},
 eventdate = {2016-04-11/2016-04-14}
}


@inproceedings{Dey.16.12.201519.12.2015,
 author = {Dey, Soumyadeep and Mukherjee, Jayanta and Sural, Shamik},
 title = {Stamp and logo detection from document images by finding outliers},
 pages = {1--4},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-8564-0},
 booktitle = {2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)},
 year = {2015-12-16/2015-12-19},
 abstract = {},
 doi = {10.1109/NCVPRIPG.2015.7489947},
 file = {http://ieeexplore.ieee.org/document/7489947/},
 file = {Stamp and Logo Detection from Document Images by Finding Outliers:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Stamp and Logo Detection from Document Images by Finding Outliers.pdf:pdf},
 eventtitle = {2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)},
 venue = {Patna, India},
 eventdate = {2015-12-16/2015-12-19}
}


@incollection{Forczmanski.2010,
 author = {Forczmański, Paweł and Frejlichowski, Dariusz},
 title = {Robust Stamps Detection and Classification by Means of General Shape Analysis},
 pages = {360--367},
 bookpagination = {page},
 volume = {6374},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-15909-1},
 series = {Lecture Notes in Computer Science},
 editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bolc, Leonard and Tadeusiewicz, Ryszard and Chmielewski, Leszek J. and Wojciechowski, Konrad},
 booktitle = {Computer Vision and Graphics},
 year = {2010},
 abstract = {},
 doi = {10.1007/978-3-642-15910-7_41},
 location = {Berlin, Heidelberg},
 file = {Robust Stamps Detection and Classification by Means of General Shape Analysis:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Robust Stamps Detection and Classification by Means of General Shape Analysis.pdf:pdf}
}


@article{Forczmanski.2015,
 author = {Forczmański, Paweł and Markiewicz, Andrzej},
 year = {2015},
 title = {Stamps Detection and Classification Using Simple Features Ensemble},
 pages = {1--15},
 pagination = {page},
 volume = {2015},
 issn = {1024-123X},
 journaltitle = {Mathematical Problems in Engineering},
 doi = {10.1155/2015/367879},
 number = {9},
 abstract = {Interesting grasp of data charecteristics},
 file = {Stamps Detection and Classification Using Simple Features Ensemlbe:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Stamps Detection and Classification Using Simple Features Ensemlbe.pdf:pdf}
}


@article{Forczmanski.2016,
 author = {Forczmański, Paweł and Markiewicz, Andrzej},
 year = {2016},
 title = {Two-stage approach to extracting visual objects from paper documents},
 pages = {1243--1257},
 pagination = {page},
 volume = {27},
 issn = {0932-8092},
 journaltitle = {Machine Vision and Applications},
 doi = {10.1007/s00138-016-0803-5},
 number = {8},
 abstract = {\textcolor[rgb]{0.1,0.1,0.1}{We apply a two-stage approach to the page segmentation.

This concept is definitely not novel in the computer vision

field; however, in this particular task is rarely used. Similar

ideas have been applied mostly to the problems of object

detection, extraction and classification in other classes of digital

images [24].}},
 file = {Two-stage approach to extracting visual objects from paper documents:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Two-stage approach to extracting visual objects from paper documents.pdf:pdf}
}


@misc{Fu.23.01.2017,
 author = {Fu, Cheng-Yang and Liu, Wei and Ranga, Ananth and Tyagi, Ambrish and Berg, Alexander C.},
 year = {23.01.2017},
 title = {DSSD : Deconvolutional Single Shot Detector},
 url = {http://arxiv.org/pdf/1701.06659v1},
 abstract = {The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with $513 \times 513$ input achieves 81.5{\%} mAP on VOC2007 test, 80.0{\%} mAP on VOC2012 test, and 33.2{\%} mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.},
 file = {http://arxiv.org/abs/1701.06659v1},
 file = {https://arxiv.org/pdf/1701.06659v1.pdf}
}


@collection{Garain.2015,
 year = {2015},
 title = {Computational Forensics},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-20124-5},
 series = {Lecture Notes in Computer Science},
 editor = {Garain, Utpal and Shafait, Faisal},
 abstract = {},
 doi = {10.1007/978-3-319-20125-2},
 location = {Cham}
}


@misc{Girshick.30.04.2015,
 author = {Girshick, Ross},
 year = {30.04.2015},
 title = {Fast R-CNN},
 url = {http://arxiv.org/pdf/1504.08083v2},
 abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
 file = {https://arxiv.org/pdf/1504.08083v2.pdf},
 file = {http://arxiv.org/abs/1504.08083v2},
 file = {Fast-RCNN:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Fast-RCNN.pdf:pdf}
}


@article{Gu.,
 author = {Gu, Jindong and Yang, Yinchong and Tresp, Volker},
 title = {Understanding Individual Decisions of CNNs via Contrastive Backpropagation},
 abstract = {A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP.},
 pagetotal = {16},
 file = {Understanding Individual Decisions of CNNs via Contrastive  Backpropagation:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Understanding Individual Decisions of CNNs via Contrastive  Backpropagation.pdf:pdf}
}


@thesis{GuancongLi.2011,
 author = {{Guancong Li}},
 year = {2011},
 title = {Postage Postage Postage Postage stamp recognition recognition recognition recognition using image processing processing processing},
 url = {http://urn.kb.se/resolve?urn=urn%3Anbn%3Ase%3Ahig%3Adiva-9425},
 urldate = {2019-04-08},
 abstract = {},
 type = {Bachelor Thesis},
 institution = {{University of Gävle}},
 location = {Gävle, Sweden},
 file = {Postage stamp recognition using image processing:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Postage stamp recognition using image processing.pdf:pdf},
 titleaddon = {Faculty of Engineering and Sustainable Development}
}


@article{Hassanzadeh.2011,
 author = {Hassanzadeh, Sina and Pourghassem, Hossein},
 year = {2011},
 title = {A Novel Logo Detection and Recognition Framework for Separated Part Logos in Document Images},
 volume = {5},
 journaltitle = {Australian Journal of Basic and Applied Sciences},
 abstract = {},
 file = {A Novel Logo Detection and Recognition Framework for Separated Part Logos in Document Images:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\A Novel Logo Detection and Recognition Framework for Separated Part Logos in Document Images.pdf:pdf}
}


@book{Hastie.2001,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
 year = {2001},
 title = {The Elements of Statistical Learning},
 edition = {2},
 publisher = {Springer},
 isbn = {978-0-387-21606-5},
 subtitle = {Data mining, inference, and prediction},
 location = {New York},
 series = {Springer series in statistics},
 abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
 file = {The Elements of Statistical Learning:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\The Elements of Statistical Learning.pdf:pdf}
}


@misc{He.20.03.2017,
 author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
 year = {20.03.2017},
 title = {Mask R-CNN},
 url = {http://arxiv.org/pdf/1703.06870v3},
 abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
 file = {https://arxiv.org/pdf/1703.06870v3.pdf},
 file = {http://arxiv.org/abs/1703.06870v3},
 file = {Mask R-CNN:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Mask R-CNN.pdf:pdf}
}


@collection{Hutchison.2010,
 year = {2010},
 title = {Computer Vision and Graphics},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-15909-1},
 series = {Lecture Notes in Computer Science},
 editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bolc, Leonard and Tadeusiewicz, Ryszard and Chmielewski, Leszek J. and Wojciechowski, Konrad},
 abstract = {},
 doi = {10.1007/978-3-642-15910-7},
 location = {Berlin, Heidelberg}
}


@book{IanGoodfellow.2016,
 author = {{Ian Goodfellow} and {Yoshua Bengio} and {Aaron Courville}},
 year = {2016},
 title = {Deep Learning},
 publisher = {{MIT Press}},
 abstract = {}
}


@book{James.2017,
 author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
 year = {2017},
 title = {An introduction to statistical learning},
 edition = {Corrected at 8th printing},
 publisher = {Springer},
 isbn = {1461471370},
 subtitle = {With applications in R},
 language = {eng},
 location = {New York and Heidelberg and Dordrecht and London},
 series = {Springer texts in statistics},
 abstract = {},
 pagetotal = {426},
 note = {James, Gareth (VerfasserIn)
Witten, Daniela (VerfasserIn)
Hastie, Trevor (VerfasserIn)
Tibshirani, Robert (VerfasserIn)}
}


@inproceedings{Jeatrakul.20.10.200922.10.2009,
 author = {Jeatrakul, P. and Wong, K. W.},
 title = {Comparing the performance of different neural networks for binary classification problems},
 pages = {111--115},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4244-4138-9},
 booktitle = {2009 Eighth International Symposium on Natural Language Processing},
 year = {2009-10-20/2009-10-22},
 abstract = {},
 file = {Comparing the Performance of Different Neural Networks for Binary Classification Problems:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Comparing the Performance of Different Neural Networks for Binary Classification Problems.pdf:pdf},
 eventtitle = {2009 Eighth International Symposium on Natural Language Processing (SNLP)},
 venue = {Bangkok, Thailand},
 eventdate = {2009-10-20/2009-10-22}
}


@misc{JosephRedmon.20132016,
 author = {{Joseph Redmon}},
 year = {2013/2016},
 title = {Darknet: Open Source Neural Networks in C},
 abstract = {},
 note = {http://pjreddie.com/darknet/}
}


@online{KevinShen.2018,
 author = {{Kevin Shen}},
 editor = {{Kevin Shen}},
 year = {2018},
 title = {Effect of batch size on training dynamics},
 url = {https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e},
 urldate = {2019-04-15},
 abstract = {},
 organization = {{Mini Distill}}
}


@article{Krishnamoorthy.1993,
 author = {Krishnamoorthy, M. and Nagy, G. and Seth, S. and Viswanathan, M.},
 year = {1993},
 title = {Syntactic segmentation and labeling of digitized pages from technical journals},
 pages = {737--747},
 pagination = {page},
 volume = {15},
 issn = {01628828},
 journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
 doi = {10.1109/34.221173},
 number = {7},
 abstract = {}
}


@misc{Lapuschkin.25.08.2017,
 author = {Lapuschkin, Sebastian and Binder, Alexander and Müller, Klaus-Robert and Samek, Wojciech},
 year = {25.08.2017},
 title = {Understanding and Comparing Deep Neural Networks for Age and Gender Classification},
 abstract = {Recently, deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. However, these models were applied in a black-box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing, model initialization and architecture choice. We present a study investigating these different effects.  In detail, our work compares four popular neural network architectures, studies the effect of pretraining, evaluates the robustness of the considered alignment preprocessings via cross-method test set swapping and intuitively visualizes the model's prediction strategies in given preprocessing conditions using the recent Layer-wise Relevance Propagation (LRP) algorithm. Our evaluations on the challenging Adience benchmark show that suitable parameter initialization leads to a holistic perception of the input, compensating artefactual data representations. With a combination of simple preprocessing steps, we reach state of the art performance in gender recognition.},
 pagetotal = {8},
 file = {Understanding and Comparing Deep Neural Networks for Age and Gender Classification:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Understanding and Comparing Deep Neural Networks for Age and Gender Classification.pdf:pdf}
}


@inproceedings{Lecun.1995,
 author = {Lecun, Yann and Jackel, Larry and Bottou, L. and Brunot, A. and Cortes, Corinna and Denker, John and Drucker, Harris and Guyon, Isabelle and Muller, Urs and Sackinger, E. and Simard, Patrice and Vapnik, V.},
 title = {Comparison of learning algorithms for handwritten digit recognition},
 year = {1995},
 abstract = {},
 file = {Comparison of learning algorithms for handwritten digit recognition:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Comparison of learning algorithms for handwritten digit recognition.pdf:pdf}
}


@online{LeiMao.2019,
 author = {{Lei Mao}},
 year = {2019},
 title = {Bounding Box Encoding and Decoding in Object Detection},
 url = {leimao.github.io/blog/Bounding-Box-Encoding-Decoding/},
 urldate = {2019-04-16},
 abstract = {}
}


@article{Liu.2016,
 author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
 year = {2016},
 title = {SSD: Single Shot MultiBox Detector},
 url = {http://arxiv.org/pdf/1512.02325v5},
 pages = {21--37},
 pagination = {page},
 volume = {9905},
 issn = {0302-9743},
 journaltitle = {0302-9743},
 doi = {10.1007/978-3-319-46448-0_2},
 abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\times 300$ input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\times 500$ input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
 file = {SSD:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\SSD.pdf:pdf}
}


@inproceedings{Micenkova.18.09.201121.09.2011,
 author = {Micenková, Barbora and {van Beusekom}, Joost},
 title = {Stamp Detection in Color Document Images},
 pages = {1125--1129},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4577-1350-7},
 booktitle = {2011 International Conference on Document Analysis and Recognition},
 year = {2011-09-18/2011-09-21},
 abstract = {Dataset:



D-Star:

For evaluation of D-StaR, we used a publicly available stamp detection and verification dataset1 [4]. This dataset contains 400 document images scanned at 200, 300, and 600 dpi resolution. The scanned document images contain printed text, stamps (textual and non-textual), logos, and signatures. The dataset contains stamps of varying sizes, shapes, and colors. 341 (out of 400) scanned document images contain single or multiple stamps and remaining 59 images have no stamps. Out of 341 scanned document images, 80 contain black stamps, and remaining 241 contain colored stamps. In 55 scanned document images stamps are overlapped with text, logos, and/ or signatures. For every scanned image, there are two ground truth images available; one containing the pixel level information and the other containing the bounding box information for each stamp. So, this dataset can be used for both region classification and pixel level evaluation.



A generic framework:

The presented method is evaluated on a publicly available dataset (StaVer1) for stamp

detection and verication [19]. This dataset contains 400 scanned document images. Out

of these 400 documents, 80 documents contain black stamps whereas the remaining 320

documents contain colored stamps. All of these document images are available in 200,

300, and 600 dpi. For each image, two di

erent types of ground truths are available.

One contains the pixel level ground truth, which means all of the pixels which belong to

stamps are marked in the image. The other ground truth format contains bounding box

information for each stamp. Hence, this dataset can be used for both pixel level as well

as patch level evaluation of stamp detection. In addition, it contains di

erent types of

stamps ranging from rectangular, oval, to irregular shaped, and most importantly, textual

stamps.



Stamp and logo detect from outlier:

The proposed approach is applied on publicly available

StampVer dataset [8]. The dataset comprises of stamps and

logos in different shapes, sizes and color. Both textual and

non-textual stamps with different orientation are present in the

dataset. There are 400 images in the dataset, and all the images

are available in 200, 300, and 600 dpi resolutions. Out of 400

images, 59 images contain no stamp region, and in the remaining

341 images, 261 contain only color stamps, and the rest 80

contain mixture of black and color stamps. Further, StampVer

dataset contains 55 images with overlapped stamp regions, out

of which 52 images are with color stamps. Groundtruth for

only stamp regions is available for the public dataset [8]. We

have generated groundtruth by annotating all the components

of 344 images with non-overlapping components for 300 dpi

resolution images from the StampVer dataset. The annotated

dataset is available online1. We use this groundtruth as well

as publicly available stamp groundtruth [8] for evaluation of

the algorithm for stamp and logo classification of images with

300 dpi resolution.},
 doi = {10.1109/ICDAR.2011.227},
 file = {http://ieeexplore.ieee.org/document/6065485/},
 file = {Stamp detection in Color Document Images:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Stamp detection in Color Document Images.pdf:pdf},
 eventtitle = {2011 International Conference on Document Analysis and Recognition (ICDAR)},
 venue = {Beijing, China},
 eventdate = {2011-09-18/2011-09-21}
}


@incollection{Micenkova.2015,
 author = {Micenková, Barbora and {van Beusekom}, Joost and Shafait, Faisal},
 title = {Stamp Verification for Automated Document Authentication},
 pages = {117--129},
 bookpagination = {page},
 volume = {8915},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-20124-5},
 series = {Lecture Notes in Computer Science},
 editor = {Garain, Utpal and Shafait, Faisal},
 booktitle = {Computational Forensics},
 year = {2015},
 abstract = {Stamps, along with signatures, can be considered as the most widely used extrinsic security feature in paper documents. In contrast to signatures, however, for stamps little work has been done to automatically verify their authenticity. 



In this paper, an approach for verication of color stamps is presented. We focus on photocopied stamps as non-genuine stamps. Our previously presented stamp detection method is improved and extended to verify that the stamp is genuine and not a copy. Using a variety of features, a classier is trained that allows successful separation between genuine stamps and copied stamps. 



Sensitivity and specicity of up to 95{\%} could be obtained on a data set that is publicly available.},
 doi = {10.1007/978-3-319-20125-2_11},
 location = {Cham},
 file = {Stamp Verification for Automated Document Authentication:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Stamp Verification for Automated Document Authentication.pdf:pdf}
}


@book{MichaelA.Nielsen.2015,
 author = {{Michael A. Nielsen}},
 year = {2015},
 title = {Neural Networks and Deep Learning},
 publisher = {{Determination Press}},
 abstract = {}
}


@article{Montavon.2017,
 author = {Montavon, Grégoire and Bach, Sebastian and Binder, Alexander and Samek, Wojciech and Müller, Klaus-Robert},
 year = {2017},
 title = {Explaining NonLinear Classification Decisions with Deep Taylor Decomposition},
 pages = {211--222},
 pagination = {page},
 volume = {65},
 issn = {00313203},
 journaltitle = {Pattern Recognition},
 abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems, e.g., image classification, natural language processing or human action recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method is based on deep Taylor decomposition and efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
 pagetotal = {20},
 file = {Explaining NonLinear Classification Decisions with Deep Taylor  Decomposition:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Explaining NonLinear Classification Decisions with Deep Taylor  Decomposition.pdf:pdf}
}


@online{na.2016,
 author = {n/a},
 year = {2016},
 title = {Tutorial: Implementing Deep Taylor Decomposition / LRP},
 url = {http://heatmapping.org/tutorial/},
 urldate = {2019-04-15},
 abstract = {}
}


@online{na.2017,
 author = {n/a},
 year = {2017},
 title = {A Quick Introduction to Deep Taylor Decomposition},
 url = {http://www.heatmapping.org/deeptaylor/},
 urldate = {2019-04-15},
 abstract = {}
}


@inproceedings{Nandedkar.16.12.201519.12.2015,
 author = {Nandedkar, Amit V. and Mukherjee, Jayanta and Sural, Shamik},
 title = {A spectral filtering based deep learning for detection of logo and stamp},
 pages = {1--4},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-8564-0},
 booktitle = {2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)},
 year = {2015-12-16/2015-12-19},
 abstract = {The presence of multiple colors

is predominant in case of logo class. Whereas, the stamps are

made up of a single color. This characteristic is exploited by

deep convolutional neural network for differentiating stamps

and logos.



This would be an interesting task for the innvestigate framework},
 doi = {10.1109/NCVPRIPG.2015.7490053},
 file = {http://ieeexplore.ieee.org/document/7490053/},
 file = {A Spectral Filtering Based Deep Learning for Detection of Logo and Stamp:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\A Spectral Filtering Based Deep Learning for Detection of Logo and Stamp.pdf:pdf},
 eventtitle = {2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)},
 venue = {Patna, India},
 eventdate = {2015-12-16/2015-12-19}
}


@inproceedings{Nandedkar.23.08.201526.08.2015,
 author = {Nandedkar, Amit Vijay and Mukhopadhyay, Jayanta and Sural, Shamik},
 title = {Text-graphics separation to detect logo and stamp from color document images: A spectral approach},
 pages = {571--575},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4799-1805-8},
 booktitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
 year = {2015-08-23/2015-08-26},
 abstract = {Shows that text is the major source of spatial frequency components in

a document image

The

main contribution of this work is to separate text and graphics

regions from document image without performing complex operations,

such as text-size dependent block segmentation [9][

13], projection profile analysis [17], the supervised training

[14, 15] for classification, etc. It uses the simple fact that

spectral characteristics of textual regions differ from most of

the graphical regions. It is a non-filter bank based approach

[18] followed by a mean-shift segmentation [19] processing to

separate textual and graphical regions in the document image.},
 doi = {10.1109/ICDAR.2015.7333826},
 file = {http://ieeexplore.ieee.org/document/7333826/},
 file = {Text-graphics separation to detect logo and stamp from color document images - A spectral approach:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Text-graphics separation to detect logo and stamp from color document images - A spectral approach.pdf:pdf},
 eventtitle = {2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
 venue = {Tunis, Tunisia},
 eventdate = {2015-08-23/2015-08-26}
}


@inproceedings{Petej.07.07.201310.07.2013,
 author = {Petej, Pjero and Gotovac, Sven},
 title = {Comparison of stamp classification using SVM and random ferns},
 pages = {000850--000854},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4799-3755-4},
 booktitle = {2013 IEEE Symposium on Computers and Communications (ISCC)},
 year = {2013-07-07/2013-07-10},
 abstract = {},
 doi = {10.1109/ISCC.2013.6755055},
 file = {http://ieeexplore.ieee.org/document/6755055/},
 file = {Comparison of stamp classification using SVM and Random ferns:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Comparison of stamp classification using SVM and Random ferns.pdf:pdf},
 eventtitle = {2013 IEEE Symposium on Computers and Communications (ISCC)},
 venue = {Split, Croatia},
 eventdate = {2013-07-07/2013-07-10}
}


@misc{Redmon.08.06.2015,
 author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
 year = {08.06.2015},
 title = {You Only Look Once: Unified, Real-Time Object Detection},
 url = {http://arxiv.org/pdf/1506.02640v5},
 abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
 file = {http://arxiv.org/abs/1506.02640v5},
 file = {https://arxiv.org/pdf/1506.02640v5.pdf},
 file = {YOLO:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\YOLO.pdf:pdf}
}


@misc{Redmon.09.04.2018,
 author = {Redmon, Joseph and Farhadi, Ali},
 year = {09.04.2018},
 title = {YOLOv3: An Incremental Improvement},
 url = {http://arxiv.org/pdf/1804.02767v1},
 abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
 file = {http://arxiv.org/abs/1804.02767v1},
 file = {https://arxiv.org/pdf/1804.02767v1.pdf}
}


@misc{Redmon.25.12.2016,
 author = {Redmon, Joseph and Farhadi, Ali},
 year = {25.12.2016},
 title = {YOLO9000: Better, Faster, Stronger},
 url = {http://arxiv.org/pdf/1612.08242v1},
 abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
 file = {http://arxiv.org/abs/1612.08242v1},
 file = {https://arxiv.org/pdf/1612.08242v1.pdf},
 file = {YOLO9000:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\YOLO9000.pdf:pdf}
}


@misc{Ren.04.06.2015,
 author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
 year = {04.06.2015},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
 url = {http://arxiv.org/pdf/1506.01497v3},
 abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
 file = {https://arxiv.org/pdf/1506.01497v3.pdf},
 file = {http://arxiv.org/abs/1506.01497v3},
 file = {Faster-RCNN:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Faster-RCNN.pdf:pdf}
}


@misc{Selvaraju.07.10.2016,
 author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
 year = {07.10.2016},
 title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
 abstract = {We propose a technique for producing {\textquotedbl}visual explanations{\textquotedbl} for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a {\textquotedbl}stronger{\textquotedbl} deep network from a {\textquotedbl}weaker{\textquotedbl} one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.},
 pagetotal = {24},
 file = {Grad-CAM - Visual Explanations from Deep Networks via Gradient-based  Localization:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Grad-CAM - Visual Explanations from Deep Networks via Gradient-based  Localization.pdf:pdf}
}


@online{Shiebler.2017,
 author = {Shiebler, Dan},
 year = {2017},
 title = {Understanding Neural Networks with Layerwise Relevance Propagation and Deep Taylor Series},
 url = {http://danshiebler.com/2017-04-16-deep-taylor-lrp/},
 urldate = {2019-04-15},
 abstract = {}
}


@article{Siegelmann.1995,
 author = {Siegelmann, H. T. and Sontag, E. D.},
 year = {1995},
 title = {On the Computational Power of Neural Nets},
 pages = {132--150},
 pagination = {page},
 volume = {50},
 issn = {00220000},
 journaltitle = {Journal of Computer and System Sciences},
 doi = {10.1006/jcss.1995.1013},
 number = {1},
 abstract = {}
}


@proceedings{Taghva.2006,
 year = {2006},
 title = {Document Recognition and Retrieval XIII},
 publisher = {SPIE},
 series = {SPIE Proceedings},
 editor = {Taghva, Kazem and Lin, Xiaofan},
 abstract = {},
 venue = {San Jose, CA},
 eventdate = {2006-01-15},
 eventtitle = {Electronic Imaging 2006}
}


@inproceedings{Viola.814Dec.2001,
 author = {Viola, P. and Jones, M.},
 title = {Rapid object detection using a boosted cascade of simple features},
 pages = {I-511-I-518},
 bookpagination = {page},
 publisher = {{IEEE Comput. Soc}},
 isbn = {0-7695-1272-0},
 booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
 year = {2001-12-08/2001-12-14},
 abstract = {},
 doi = {10.1109/CVPR.2001.990517},
 file = {http://ieeexplore.ieee.org/document/990517/},
 file = {Rapid object detection using a boosted cascade of simple features:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Rapid object detection using a boosted cascade of simple features.pdf:pdf},
 eventtitle = {2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
 venue = {Kauai, HI, USA},
 eventdate = {2001-12-08/2001-12-14}
}


@article{Wang.2016,
 author = {Wang, Fanglin and Qi, Shuhan and Gao, Ge and Zhao, Sicheng and Wang, Xiangyu},
 year = {2016},
 title = {Logo information recognition in large-scale social media data},
 pages = {63--73},
 pagination = {page},
 volume = {22},
 issn = {0942-4962},
 journaltitle = {Multimedia Systems},
 doi = {10.1007/s00530-014-0393-x},
 number = {1},
 abstract = {}
}


@misc{Weiss.13.05.2018,
 author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
 year = {13.05.2018},
 title = {On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
 abstract = {While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.},
 file = {https://arxiv.org/pdf/1805.04908v1.pdf}
}


@inproceedings{Younas.09.11.201715.11.2017,
 author = {Younas, Junaid and Afzal, Muhammad Zeshan and Malik, Muhammad Imran and Shafait, Faisal and Lukowicz, Paul and Ahmed, Sheraz},
 title = {D-StaR: A Generic Method for Stamp Segmentation from Document Images},
 pages = {248--253},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5386-3586-5},
 booktitle = {2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
 year = {2017-11-09/2017-11-15},
 abstract = {Todo:

Recently, a shape specific stamp segmenting approach

using examplar features is proposed by Bhalgat et al. [4].

This approach uses unsupervised learning methods to extract

the dictionary items for stamp shapes. Feature vectors are

extracted by using single Convolution layer with 4 \textit{× }4

quadrant maxpooling. Dictionary ranking item scheme is

used for recognition of stamps. This approach produces

excellent results for only oval shaped stamps.},
 doi = {10.1109/ICDAR.2017.49},
 file = {http://ieeexplore.ieee.org/document/8269980/},
 file = {D-StaR - A Generic Method for Stamp Segmentation from Document Images:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\D-StaR - A Generic Method for Stamp Segmentation from Document Images.pdf:pdf},
 eventtitle = {2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
 venue = {Kyoto},
 eventdate = {2017-11-09/2017-11-15}
}


@misc{Yu.21.01.2018,
 author = {Yu, Tao and Long, Huan and Hopcroft, John E.},
 year = {21.01.2018},
 title = {Curvature-based Comparison of Two Neural Networks},
 abstract = {In this paper we show the similarities and differences of two deep neural networks by comparing the manifolds composed of activation vectors in each fully connected layer of them. The main contribution of this paper includes 1) a new data generating algorithm which is crucial for determining the dimension of manifolds; 2) a systematic strategy to compare manifolds. Especially, we take Riemann curvature and sectional curvature as part of criterion, which can reflect the intrinsic geometric properties of manifolds. Some interesting results and phenomenon are given, which help in specifying the similarities and differences between the features extracted by two networks and demystifying the intrinsic mechanism of deep neural networks.},
 file = {Curvature-based Comparison of Two Neural Networks:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Curvature-based Comparison of Two Neural Networks.pdf:pdf}
}


@misc{Zeiler.12.11.2013,
 author = {Zeiler, Matthew D. and Fergus, Rob},
 year = {12.11.2013},
 title = {Visualizing and Understanding Convolutional Networks},
 abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
 file = {Visualizing and Understanding Convolutional Networks:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\Visualizing and Understanding Convolutional Networks.pdf:pdf}
}


@inproceedings{Zhu.2006,
 author = {Zhu, Guangyu and Jaeger, Stefan and Doermann, David},
 title = {A robust stamp detection framework on degraded documents},
 pages = {60670B-60670B-9},
 bookpagination = {page},
 publisher = {SPIE},
 series = {SPIE Proceedings},
 editor = {Taghva, Kazem and Lin, Xiaofan},
 booktitle = {Document Recognition and Retrieval XIII},
 year = {2006},
 abstract = {},
 doi = {10.1117/12.643537},
 file = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=729016},
 file = {A robust stamp detection framework on degraded documents:C\:\\Users\\zietl\\Documents\\_Eigene Dokumente\\Uni Repositories\\tf3tk\\Citavi\\TF3TK\\Citavi Attachments\\A robust stamp detection framework on degraded documents.pdf:pdf},
 eventtitle = {Electronic Imaging 2006},
 venue = {San Jose, CA},
 eventdate = {2006-01-15}
}


@inproceedings{Zhu.23.09.200726.09.2007,
 author = {Zhu, G. and Doermann, D.},
 title = {Automatic Document Logo Detection},
 pages = {864--868},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {0-7695-2822-8},
 booktitle = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2},
 year = {2007-09-23/2007-09-26},
 abstract = {},
 doi = {10.1109/ICDAR.2007.4377038},
 file = {http://ieeexplore.ieee.org/document/4377038/},
 eventtitle = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2},
 venue = {Curitiba, Parana, Brazil},
 eventdate = {2007-09-23/2007-09-26}
}


